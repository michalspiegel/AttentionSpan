{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipykernel transformer_lens plotly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens\n",
    "import torch\n",
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert HF Transformers weights to HookedTransformer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformer_lens.pretrained.weight_conversions.llama import convert_llama_weights\\nfrom transformers import AutoConfig, AutoModelForCausalLM\\n\\nconfig = AutoConfig.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\\nmodel = AutoModelForCausalLM.from_config(config)\\nmodel.load_state_dict(torch.load(\"checkpoints/llama_instruct_addition_finetuned_shorted_OOD.weights\", weights_only=True))\\nhooked_config = transformer_lens.loading.get_pretrained_model_config(\"meta-llama/Llama-3.2-1B-Instruct\")\\nnew_state_dict = convert_llama_weights(model, hooked_config)\\ntorch.save(new_state_dict, \"checkpoints/llama_instruct_addition_finetuned_hooked.weights\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformer_lens.pretrained.weight_conversions.llama import convert_llama_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "model.load_state_dict(torch.load(\"checkpoints/llama_instruct_addition_finetuned_shorted_OOD.weights\", weights_only=True))\n",
    "hooked_config = transformer_lens.loading.get_pretrained_model_config(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "new_state_dict = convert_llama_weights(model, hooked_config)\n",
    "torch.save(new_state_dict, \"checkpoints/llama_instruct_addition_finetuned_hooked.weights\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    }
   ],
   "source": [
    "config = transformer_lens.loading.get_pretrained_model_config(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = transformer_lens.HookedTransformer(config)\n",
    "model.load_and_process_state_dict(torch.load(\"checkpoints/llama_instruct_addition_finetuned_hooked.weights\", weights_only=True))\n",
    "#config = transformer_lens.loading.get_pretrained_model_config(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "#model = transformer_lens.HookedTransformer(config)\n",
    "#model.load_and_process_state_dict(torch.load(\"checkpoints/Qwen2.5-1.5B-Instruct_value_assign_finetuned_hooked.weights\", weights_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0-15): 16 x TransformerBlock(\n",
      "      (ln1): RMSNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): RMSNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): GroupedQueryAttention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "        (hook_rot_k): HookPoint()\n",
      "        (hook_rot_q): HookPoint()\n",
      "      )\n",
      "      (mlp): GatedMLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_pre_linear): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_mid): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): RMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "charset_from = \"\"\n",
    "for id in range(0, 50000):\n",
    "    token = model.tokenizer.decode(id, skip_special_tokens=True)\n",
    "    if len(token) > 1 or token == \"ï¿½\":\n",
    "        continue\n",
    "    decode_id = model.tokenizer.encode(token, add_special_tokens=False)\n",
    "    if len(decode_id) == 1 and decode_id[0] == id:\n",
    "        charset_from += token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charset_from[:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "sys.path.insert(0, \"../generators\")\n",
    "sys.path.insert(0, \"generators\")\n",
    "\n",
    "\n",
    "from generators.long_addition import LongAdditionGenerator\n",
    "import transformers\n",
    "import itertools\n",
    "\n",
    "\n",
    "data_generator = LongAdditionGenerator(tokenizer=model.tokenizer,\n",
    "                                       seed=0,\n",
    "                                       use_few_shot=True, \n",
    "                                       use_instruction=True, \n",
    "                                       apply_chat_template=True,\n",
    "                                       num_of_nums=(2, 2),\n",
    "                                       length=(4, 4)\n",
    "                                       )\n",
    "samples = itertools.islice(data_generator.generate_samples(), 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(samples)\n",
    "texts = [sample[0] for sample in samples]\n",
    "input_ids = [sample[1] for sample in samples]\n",
    "target_ids = [sample[2] for sample in samples]\n",
    "attn_labels = [sample[3] for sample in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 18 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\\nYou are an expert in solving arithmetic tasks. You will be tasked with computing the sum of input numbers.\\nThe number are all represented with their least significant digit first.\\nSum these number using the standard addition algorithm. When predicting each digit of the result, always sum the corresponding digits of each number and remember the carry.\\nDo not generate any words. You can only generate number and nothing else.\\n\\nHere are some examples:\\n1+1=2\\n1240+4335+3440=8916\\n999+999=8991\\nUse them to solve the following task:\\n36610+67390=<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n93011<|eot_id|>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def visualize_attention_heads(attention_matrix, tokens=None, title=\"Attention Heatmap\", save_path=None, heads=None, length=None, attn_labels=None):\n",
    "    \"\"\"\n",
    "    Visualize a multi-head attention matrix as heatmaps for each head.\n",
    "\n",
    "    Parameters:\n",
    "        attention_matrix (torch.Tensor): Tensor of shape (1, num_heads, seq_len, seq_len).\n",
    "        tokens (list, optional): List of tokens corresponding to the rows/columns of the matrix.\n",
    "        title (str): Title of the heatmap.\n",
    "        save_path (str, optional): Path to save the heatmap images. If None, the heatmaps are not saved.\n",
    "        heads (list, optional): List of head indices to visualize.\n",
    "        length (int, optional): Number of tokens to display from the lower-right corner.\n",
    "\"\"\"\n",
    "    scores = attention_matrix.squeeze(0).cpu().detach().numpy()  # Shape: (num_heads, seq_len, seq_len)\n",
    "    num_heads = scores.shape[0]\n",
    "    seq_len = scores.shape[1]\n",
    "\n",
    "    for head in range(num_heads):\n",
    "        if heads is not None and head not in heads:\n",
    "            continue\n",
    "        head_scores = scores[head]  # Shape: (seq_len, seq_len)\n",
    "\n",
    "        # Apply filtering for the lower-right \"length x length\" square\n",
    "        if length is not None and length < seq_len:\n",
    "            head_scores = head_scores[-length:, -length:]\n",
    "            if tokens:\n",
    "                tokens = tokens[-length:]\n",
    "            if attn_labels is not None:\n",
    "                attn_labels = attn_labels[-length:, -length:]\n",
    "\n",
    "        # Normalize scores for better visualization\n",
    "        min_score = head_scores.min()\n",
    "        max_score = head_scores.max()\n",
    "        normalized = (head_scores - min_score) / (max_score - min_score)  # Normalize to [0, 1]\n",
    "        print(\"fig\")\n",
    "        # Create the heatmap\n",
    "        fig = go.Figure(\n",
    "            data=go.Heatmap(\n",
    "                z=normalized,\n",
    "                x=tokens if tokens else list(range(len(normalized))),\n",
    "                y=tokens if tokens else list(range(len(normalized))),\n",
    "                colorscale=\"Viridis\",\n",
    "                colorbar=dict(title=\"Attention Score\"),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Add title and layout\n",
    "        fig.update_layout(\n",
    "            title=f\"{title} - Head {head + 1}\",\n",
    "            xaxis=dict(title=\"Tokens\", tickangle=45),\n",
    "            yaxis=dict(title=\"Tokens\"),\n",
    "        )\n",
    "        print(\"attn\")\n",
    "        # Highlight attention label regions if provided\n",
    "        if attn_labels is not None:\n",
    "            for i in range(len(attn_labels)):\n",
    "                for j in range(len(attn_labels)):\n",
    "                    if attn_labels[i, j] == 1:\n",
    "                        fig.add_shape(\n",
    "                            type=\"rect\",\n",
    "                            x0=j - 0.5,\n",
    "                            x1=j + 0.5,\n",
    "                            y0=i - 0.5,\n",
    "                            y1=i + 0.5,\n",
    "                            line=dict(color=\"red\", width=2),\n",
    "                        )\n",
    "        print(\"finish\")\n",
    "        # Save the plot if a save path is provided\n",
    "        if save_path:\n",
    "            fig.write_image(f\"{save_path}_head_{head + 1}.png\")\n",
    "\n",
    "        # Show the plot\n",
    "        #fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "0.002\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "Loss: 0.6983170509338379, Accuracy: 66.67%\n",
      "Moving model to device:  cuda\n",
      "0.002\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "Loss: 0.7976048588752747, Accuracy: 66.67%\n",
      "Moving model to device:  cuda\n",
      "0.002\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "Loss: 0.7739169001579285, Accuracy: 66.67%\n",
      "Moving model to device:  cuda\n",
      "0.002\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "Loss: 0.5149678587913513, Accuracy: 66.67%\n",
      "Moving model to device:  cuda\n",
      "0.002\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "Loss: 0.6661269664764404, Accuracy: 83.33%\n",
      "Moving model to device:  cuda\n",
      "0.002\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "Loss: 0.5401667952537537, Accuracy: 83.33%\n",
      "Moving model to device:  cuda\n",
      "0.002\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "Loss: 1.2219306230545044, Accuracy: 66.67%\n",
      "Moving model to device:  cuda\n",
      "0.002\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "Loss: 0.5922104716300964, Accuracy: 83.33%\n",
      "Moving model to device:  cuda\n",
      "0.002\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "Loss: 1.1297663450241089, Accuracy: 66.67%\n",
      "Moving model to device:  cuda\n",
      "0.002\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "Loss: 0.962644100189209, Accuracy: 50.00%\n",
      "Moving model to device:  cuda\n",
      "0.002\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "Loss: 2.07841157913208, Accuracy: 50.00%\n",
      "Moving model to device:  cuda\n",
      "0.002\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n",
      "fig\n",
      "attn\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "best_heads = {}\n",
    "\n",
    "def compute_scores(logits, target):\n",
    "    ignore_index = -100  # Optional: index to ignore in loss/accuracy computation\n",
    "    # Compute loss\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "    logits.view(-1, logits.size(-1)),  # Flatten logits to (batch_size * seq_len, vocab_size)\n",
    "    target.view(-1),               # Flatten target_ids to (batch_size * seq_len)\n",
    "    ignore_index=ignore_index          # Ignore padding tokens if applicable\n",
    ")\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = torch.argmax(logits, dim=-1)  # Get predicted token indices\n",
    "    correct = (predictions == target) & (target != ignore_index)  # Ignore padding tokens\n",
    "    accuracy = correct.sum().item() / (target != ignore_index).sum().item()\n",
    "\n",
    "    print(f\"Loss: {loss.item()}, Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    errors = (predictions != target) & (target != ignore_index)  # Identify error positions\n",
    "    errors_np = errors.cpu().numpy()  # Convert to NumPy for visualization\n",
    "    #print(\"Errors:\", errors_np)\n",
    "\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "def fix_attn_pattern_hook(\n",
    "    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint,\n",
    "    attn_labels_1_idx,\n",
    "    heads_to_reinforce,\n",
    "    attn_labels,\n",
    "    keep_last_n_tokens,\n",
    "    threshold=3e-3\n",
    ") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
    "    batch, heads, seq_len, _ = value.shape \n",
    "    batch, heads, seq_len, _ = value.shape\n",
    "    layer = int(hook.name.split(\".\")[1])  # Extract layer number from hook name\n",
    "\n",
    "    if layer not in heads_to_reinforce.keys():\n",
    "        return value\n",
    "\n",
    "    # Create a mask from attn_labels_1_idx\n",
    "    mask = torch.zeros((seq_len, seq_len), dtype=torch.bool, device=value.device)\n",
    "    indices_tensor = torch.tensor(list(attn_labels_1_idx), dtype=torch.long, device=value.device)\n",
    "    mask[indices_tensor[:, 0], indices_tensor[:, 1]] = True\n",
    "\n",
    "    # Apply the mask and threshold for each head\n",
    "    for h in range(heads):\n",
    "        if h not in heads_to_reinforce[layer]:\n",
    "            continue\n",
    "\n",
    "        # Get the attention scores for the current head\n",
    "        head_scores = value[0, h]\n",
    "\n",
    "        # Apply the mask and threshold\n",
    "        masked_scores = head_scores[mask]\n",
    "        above_threshold = masked_scores >= threshold\n",
    "\n",
    "        # Update the attention scores for positions that meet the condition\n",
    "        head_scores[mask] += above_threshold.float()\n",
    "        visualize_attention_heads(value, None, title=f\"Example Attention Heatmap Layer {layer}\", heads=[h], length=keep_last_n_tokens, attn_labels=None, save_path=\"vis2/add\")\n",
    "        return value\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "is_attn_scores = lambda name: name.endswith(\"hook_pattern\")\n",
    "ablated = []\n",
    "for idx in range(0, 30):\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    indices =  np.argwhere(attn_labels[idx].cpu().numpy() == 1)\n",
    "    attn_labels_1_idx = set(map(tuple, indices))\n",
    "    \n",
    "\n",
    "    model.to(device)\n",
    "    input_ids[idx] = input_ids[idx].to(device)\n",
    "    target_ids[idx] = target_ids[idx].to(device)\n",
    "    attn_labels[idx] = attn_labels[idx].to(device)\n",
    "                \n",
    "\n",
    "    heads_to_reinforce_x=[\n",
    "        (0, range(32)),\n",
    "        (1, range(32)),\n",
    "        (2, range(32)),\n",
    "        (3, range(32)),\n",
    "        (4, range(32)),\n",
    "        (5, range(32)),\n",
    "        (6, range(32)),\n",
    "        (7, range(32)),\n",
    "        (8, range(32)),\n",
    "        (9, range(32)),\n",
    "        (10, range(32)),\n",
    "        (11, range(32)),\n",
    "        (12, range(32)),\n",
    "        (13, range(32)),\n",
    "        (14, range(32)),\n",
    "        (15, range(32)),\n",
    "    ]\n",
    "    threshold = 0.02\n",
    "    for threshold in np.linspace(0.002, 0.002, 1):\n",
    "        cols_to_keep = ~np.all(attn_labels[idx].cpu().numpy() == -100, axis=0)\n",
    "        keep_last_n_tokens = 400\n",
    "        heads_x = dict(heads_to_reinforce_x)\n",
    "        print(threshold)\n",
    "        logits = model.run_with_hooks(input_ids[idx], return_type=\"logits\",\n",
    "                                        fwd_hooks=[\n",
    "                                            (is_attn_scores, partial(fix_attn_pattern_hook, attn_labels_1_idx=attn_labels_1_idx, heads_to_reinforce=heads_x, attn_labels=attn_labels[idx], keep_last_n_tokens = keep_last_n_tokens, threshold=threshold))\n",
    "                                        ])\n",
    "        loss, acc = compute_scores(logits, target_ids[idx])\n",
    "        ablated.append((heads_x, loss, acc))\n",
    "        del logits\n",
    "        torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = 0\n",
    "for item in ablated:\n",
    "    sums += item[2]\n",
    "print(sums/len(ablated))\n",
    "print(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in sorted(best_heads.items(), key=lambda item: item[1], reverse=True):\n",
    "    print(item[0][0], item[0][1], item[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
