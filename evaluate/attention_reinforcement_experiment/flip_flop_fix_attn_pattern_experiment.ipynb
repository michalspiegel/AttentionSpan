{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (6.29.5)\n",
      "Requirement already satisfied: transformer_lens in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: plotly in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (6.0.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (8.36.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (6.4.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (1.8.14)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: packaging in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (24.2)\n",
      "Requirement already satisfied: psutil in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (6.1.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: pyzmq>=24 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (26.4.0)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: nest-asyncio in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (0.3.2)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: numpy>=1.24 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (4.67.1)\n",
      "Requirement already satisfied: transformers-stream-generator<0.0.6,>=0.0.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (0.0.5)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (3.5.1)\n",
      "Requirement already satisfied: sentencepiece in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (0.2.0)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (1.6.0)\n",
      "Requirement already satisfied: einops>=0.6.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (0.8.1)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (0.14.1)\n",
      "Requirement already satisfied: typeguard<5.0,>=4.2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (4.4.2)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (2.2.3)\n",
      "Requirement already satisfied: rich>=12.6.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (14.0.0)\n",
      "Requirement already satisfied: typing-extensions in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (4.12.2)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (0.19.6)\n",
      "Requirement already satisfied: torch>=2.2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (2.6.0)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: transformers>=4.43 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (4.50.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from plotly) (1.38.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (0.28.1)\n",
      "Requirement already satisfied: pyyaml in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (0.5.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (20.0.0)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (2025.2.0)\n",
      "Requirement already satisfied: filelock in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (3.17.0)\n",
      "Requirement already satisfied: xxhash in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (3.11.18)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
      "Requirement already satisfied: jedi>=0.16 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: exceptiongroup in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (1.2.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (2.19.1)\n",
      "Requirement already satisfied: decorator in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: stack_data in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from jaxtyping>=0.2.11->transformer_lens) (0.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.3.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from pandas>=1.1.5->transformer_lens) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from pandas>=1.1.5->transformer_lens) (2025.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
      "Requirement already satisfied: triton==3.2.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (3.2.0)\n",
      "Requirement already satisfied: jinja2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (3.1.5)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (0.6.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (9.1.0.70)\n",
      "Requirement already satisfied: networkx in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.2->transformer_lens) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformers>=4.43->transformer_lens) (0.21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformers>=4.43->transformer_lens) (2024.11.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (5.29.3)\n",
      "Requirement already satisfied: setuptools in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (59.6.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (8.1.8)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (2.21.0)\n",
      "Requirement already satisfied: setproctitle in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (1.3.4)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (2.10.6)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (3.1.44)\n",
      "Requirement already satisfied: six>=1.4.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.17.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (0.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (25.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (5.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.20.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.4.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.6.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (2.27.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from jinja2->torch>=2.2->transformer_lens) (3.0.2)\n",
      "Requirement already satisfied: pure-eval in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (2.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipykernel transformer_lens plotly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=7\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens\n",
    "import torch\n",
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert HF Transformers weights to HookedTransformer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformer_lens.pretrained.weight_conversions.llama import convert_llama_weights\\nfrom transformers import AutoConfig, AutoModelForCausalLM\\n\\nconfig = AutoConfig.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\\nmodel = AutoModelForCausalLM.from_config(config)\\nmodel.load_state_dict(torch.load(\"checkpoints/llama_instruct_flip_flop_finetuned_grok.weights\", weights_only=True))\\nhooked_config = transformer_lens.loading.get_pretrained_model_config(\"meta-llama/Llama-3.2-1B-Instruct\")\\nnew_state_dict = convert_llama_weights(model, hooked_config)\\ntorch.save(new_state_dict, \"checkpoints/llama_instruct_flip_flop_finetuned_grok_hooked.weights\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformer_lens.pretrained.weight_conversions.llama import convert_llama_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "model.load_state_dict(torch.load(\"checkpoints/llama_instruct_flip_flop_finetuned_grok.weights\", weights_only=True))\n",
    "hooked_config = transformer_lens.loading.get_pretrained_model_config(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "new_state_dict = convert_llama_weights(model, hooked_config)\n",
    "torch.save(new_state_dict, \"checkpoints/llama_instruct_flip_flop_finetuned_grok_hooked.weights\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    }
   ],
   "source": [
    "config = transformer_lens.loading.get_pretrained_model_config(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = transformer_lens.HookedTransformer(config)\n",
    "model.load_and_process_state_dict(torch.load(\"checkpoints/llama_instruct_flip_flop_finetuned_grok_hooked.weights\", weights_only=True))\n",
    "#config = transformer_lens.loading.get_pretrained_model_config(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "#model = transformer_lens.HookedTransformer(config)\n",
    "#model.load_and_process_state_dict(torch.load(\"checkpoints/Qwen2.5-1.5B-Instruct_value_assign_finetuned_hooked.weights\", weights_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "charset_from = \"\"\n",
    "for id in range(0, 50000):\n",
    "    token = model.tokenizer.decode(id, skip_special_tokens=True)\n",
    "    if len(token) > 1 or token == \"ï¿½\":\n",
    "        continue\n",
    "    decode_id = model.tokenizer.encode(token, add_special_tokens=False)\n",
    "    if len(decode_id) == 1 and decode_id[0] == id:\n",
    "        charset_from += token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charset_from[:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "sys.path.insert(0, \"../generators\")\n",
    "sys.path.insert(0, \"generators\")\n",
    "\n",
    "\n",
    "from generators.flip_flop_languages import FlipFlopGenerator\n",
    "import transformers\n",
    "import itertools\n",
    "\n",
    "\n",
    "data_generator = FlipFlopGenerator(tokenizer=model.tokenizer,\n",
    "                                       seed=0,\n",
    "                                       use_few_shot=True, \n",
    "                                       use_instruction=True, \n",
    "                                       apply_chat_template=True,\n",
    "                                       length=(500,500),\n",
    "                                       )\n",
    "samples = itertools.islice(data_generator.generate_samples(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(samples)\n",
    "texts = [sample[0] for sample in samples]\n",
    "input_ids = [sample[1] for sample in samples]\n",
    "target_ids = [sample[2] for sample in samples]\n",
    "attn_labels = [sample[3] for sample in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 16 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\\nYou are an expert in solving tasks requiring manipulation of symbols. Your task will be to evaluate a string representing a register.\\nThe string is composed of commands to the register: write, read, ignore, flip. Each is represented by a single character (w, r, i, f)\\nDo not generate any words. You can only generate the symbols of the reversed string of symbols and nothing more.\\n\\nHere are some examples:\\nw11i11f10r10f10r11i11i11i11i11i11i11r10\\nUse them to solve the following task:\\nw21i61i20f11w60f61f50i00f21f01i20f31f20f00f61f60f31w51f60w40w60f30w21w21f01f11i01i40i10w01i00f60w00f61f41f60w51f21i51w20i41w10f20f11w21w00f10f50w00f60i01w00w10i10f01f01w10f21i10f30f01w21f30f50w60w21w41f10i51f61f21f20f50i50i50f20w61i41f20f21f30w40f20w11i51w31f50w30i50i41f40w31i61w61w40f10i51i00w50f40w00i20w31f00i30i11w60i60w00f21i40f51f51f31f10i41w10i21i50i60w20w41i30i01f10i10i31i30w41i21w30f31w21f50w51f00w10w30w31i30i01w20i11w00f30w31i10w40w11i20w41w41f51f31i30f40w21i00i40i41f51w60f60w10w20i20w61i41f40f00f30i41i61f40w50f11f41i01i61i31w10w21w01i11f50f50i30f31i00i60w00f20f41w31w01f51f01w31f60f60w61i20i20w01i00w01f20w31w01w01w50i31w31f20i00i50i20w60i00w00f50f01i50f10w30i21w00i21i21w40w41w30w60i40w60i21f00f01w50w31f30f31f20w31f40i30i51i11i30w60i00f31w61i10i20i30i60i01f30w20f20i51w51w41i50w20w21f41i01i20f00w61i41w40w01i20f40f01f41f61i31f50w00i31w41i51f30i31i50i20f30f10i31w60f60f51w20w40i61f10f20w31f21w21f51w30f20w11i61i61i50w30f50i00f00i00w41f01i31w51f31i51w21w00w20i50w01f11f11w20f01i41i20f00i00f20w21i41i31f10i40w31f30i31i00w10w30i40i41w10i10i21i01f40f21f41f51i10w40w51w11f50f41w01f20f41w51w50w41f20i51i40w60f41i40w30w40w60i40w50w61i40f30f60f41i31w10f31f20i11f11i30w01f50i61f50i20i01f40f50i01f00w00w51w60i51w11i11i11f61i01i41w40i30w40f60w20w10w50f01f01f10i21w50i01i50i11w51i21i01f21w61f01i50i21w01f01f51f60f41i61w01w41w60i60f60f11i21i31f30w60f01w20f31f40w41i60f10f50f01i21i01f21f41i11w60f11i30f30w20f11f51w50f11w61w41w51f60w11w31i21i41w20i01w61i31f60f31f11i60i30w01i00w50f41w20i30f21f10r3<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n0<|eot_id|>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def visualize_attention_heads(attention_matrix, tokens=None, title=\"Attention Heatmap\", save_path=None, heads=None, length=None, attn_labels=None):\n",
    "    \"\"\"\n",
    "    Visualize a multi-head attention matrix as heatmaps for each head.\n",
    "\n",
    "    Parameters:\n",
    "        attention_matrix (torch.Tensor): Tensor of shape (1, num_heads, seq_len, seq_len).\n",
    "        tokens (list, optional): List of tokens corresponding to the rows/columns of the matrix.\n",
    "        title (str): Title of the heatmap.\n",
    "        save_path (str, optional): Path to save the heatmap images. If None, the heatmaps are not saved.\n",
    "        heads (list, optional): List of head indices to visualize.\n",
    "        length (int, optional): Number of tokens to display from the lower-right corner.\n",
    "    \"\"\"\n",
    "    # Detach and convert to NumPy\n",
    "    scores = attention_matrix.squeeze(0).cpu().detach().numpy()  # Shape: (num_heads, seq_len, seq_len)\n",
    "    num_heads = scores.shape[0]\n",
    "    seq_len = scores.shape[1]\n",
    "\n",
    "    for head in range(num_heads):\n",
    "        if heads is not None and head not in heads:\n",
    "            continue\n",
    "        head_scores = scores[head]  # Shape: (seq_len, seq_len)\n",
    "\n",
    "        # Apply filtering for the lower-right \"length x length\" square\n",
    "        if length is not None and length < seq_len:\n",
    "            head_scores = head_scores[-length:, -length:]\n",
    "            attn_labels = attn_labels[-length:, -length:]\n",
    "\n",
    "        min_score = head_scores.min()\n",
    "        max_score = head_scores.max()\n",
    "        normalized = (head_scores - min_score) / (max_score - min_score)  # Normalize to [0, 1]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        plt.imshow(normalized, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "        plt.colorbar(label=\"Attention Score\")\n",
    "        plt.title(f\"{title} - Head {head + 1}\")\n",
    "\n",
    "        linewidth=1\n",
    "        # Draw borders for attention label regions\n",
    "        for i in range(length):\n",
    "            for j in range(length):\n",
    "                if attn_labels[i, j] == 1:\n",
    "                    # Left border\n",
    "                    if j == 0 or attn_labels[i, j-1] != 1:\n",
    "                        ax.plot([j - 0.5, j - 0.5], [i - 0.5, i + 0.5], color='red', linewidth=linewidth, alpha=1)\n",
    "                    # Right border\n",
    "                    if j == length - 1 or attn_labels[i, j+1] != 1:\n",
    "                        ax.plot([j + 0.5, j + 0.5], [i - 0.5, i + 0.5], color='red', linewidth=linewidth, alpha=1)\n",
    "                    # Top border\n",
    "                    if i == 0 or attn_labels[i-1, j] != 1:\n",
    "                        ax.plot([j - 0.5, j + 0.5], [i - 0.5, i - 0.5], color='red', linewidth=linewidth, alpha=1)\n",
    "                    # Bottom border\n",
    "                    if i == length - 1 or attn_labels[i+1, j] != 1:\n",
    "                        ax.plot([j - 0.5, j + 0.5], [i + 0.5, i + 0.5], color='red', linewidth=linewidth, alpha=1)\n",
    "\n",
    "\n",
    "        plt.xlabel(\"Tokens\")\n",
    "        plt.ylabel(\"Tokens\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(f\"{save_path}_head_{head + 1}.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "{9: [5, 7, 20, 25, 29, 30, 31], 10: [8], 11: [9, 8, 30], 13: [9, 11], 14: [24, 26, 27], 15: [1, 8, 10]}\n",
      "Loss: 0.45744937658309937, Accuracy: 50.00%\n",
      "Moving model to device:  cuda\n",
      "{9: [5, 7, 20, 25, 29, 30, 31], 10: [8], 11: [9, 8, 30], 13: [9, 11], 14: [24, 26, 27], 15: [1, 8, 10]}\n",
      "Loss: 0.8426240682601929, Accuracy: 50.00%\n",
      "Moving model to device:  cuda\n",
      "{9: [5, 7, 20, 25, 29, 30, 31], 10: [8], 11: [9, 8, 30], 13: [9, 11], 14: [24, 26, 27], 15: [1, 8, 10]}\n",
      "Loss: 0.006217836402356625, Accuracy: 100.00%\n",
      "Moving model to device:  cuda\n",
      "{9: [5, 7, 20, 25, 29, 30, 31], 10: [8], 11: [9, 8, 30], 13: [9, 11], 14: [24, 26, 27], 15: [1, 8, 10]}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m heads_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(heads_to_reinforce_x)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(heads_x)\n\u001b[0;32m---> 84\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mfwd_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_attn_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfix_attn_pattern_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_labels_1_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_labels_1_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheads_to_reinforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheads_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_last_n_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkeep_last_n_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m loss, acc \u001b[38;5;241m=\u001b[39m compute_scores(logits, target_ids[idx])\n\u001b[1;32m     89\u001b[0m ablated\u001b[38;5;241m.\u001b[39mappend((heads_x, loss, acc))\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/transformer_lens/hook_points.py:456\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_hooks\u001b[0;34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts) \u001b[38;5;28;01mas\u001b[39;00m hooked_model:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhooked_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:612\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    609\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    610\u001b[0m         )\n\u001b[0;32m--> 612\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/transformer_lens/components/transformer_block.py:160\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    153\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    154\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    156\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_post(attn_out)\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/transformer_lens/components/abstract_attention.py:261\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    259\u001b[0m pattern \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attn_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    260\u001b[0m pattern \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(torch\u001b[38;5;241m.\u001b[39misnan(pattern), torch\u001b[38;5;241m.\u001b[39mzeros_like(pattern), pattern)\n\u001b[0;32m--> 261\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_pattern\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, head_index, query_pos, key_pos]\u001b[39;00m\n\u001b[1;32m    262\u001b[0m pattern \u001b[38;5;241m=\u001b[39m pattern\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    263\u001b[0m pattern \u001b[38;5;241m=\u001b[39m pattern\u001b[38;5;241m.\u001b[39mto(v\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1806\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1804\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1806\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1809\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/transformer_lens/hook_points.py:109\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbwd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m ):  \u001b[38;5;66;03m# For a backwards hook, module_output is a tuple of (grad,) - I don't know why.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     module_output \u001b[38;5;241m=\u001b[39m module_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 43\u001b[0m, in \u001b[0;36mfix_attn_pattern_hook\u001b[0;34m(value, hook, attn_labels_1_idx, heads_to_reinforce, attn_labels, keep_last_n_tokens)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (i, j) \u001b[38;5;129;01min\u001b[39;00m attn_labels_1_idx:\n\u001b[1;32m     44\u001b[0m             value[\u001b[38;5;241m0\u001b[39m, h, i, j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;66;03m#acc += value[0, h, i, j]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_heads = {}\n",
    "def compute_scores(logits, target):\n",
    "    ignore_index = -100  # Optional: index to ignore in loss/accuracy computation\n",
    "    # Compute loss\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "    logits.view(-1, logits.size(-1)),  # Flatten logits to (batch_size * seq_len, vocab_size)\n",
    "    target.view(-1),               # Flatten target_ids to (batch_size * seq_len)\n",
    "    ignore_index=ignore_index          # Ignore padding tokens if applicable\n",
    ")\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = torch.argmax(logits, dim=-1)  # Get predicted token indices\n",
    "    correct = (predictions == target) & (target != ignore_index)  # Ignore padding tokens\n",
    "    accuracy = correct.sum().item() / (target != ignore_index).sum().item()\n",
    "\n",
    "    print(f\"Loss: {loss.item()}, Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    errors = (predictions != target) & (target != ignore_index)  # Identify error positions\n",
    "    errors_np = errors.cpu().numpy()  # Convert to NumPy for visualization\n",
    "    #print(\"Errors:\", errors_np)\n",
    "\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "def fix_attn_pattern_hook(\n",
    "    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint,\n",
    "    attn_labels_1_idx,\n",
    "    heads_to_reinforce,\n",
    "    attn_labels,\n",
    "    keep_last_n_tokens,\n",
    ") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
    "    batch, heads, seq_len, _ = value.shape \n",
    "    layer = int(hook.name.split(\".\")[1])\n",
    "    #if layer not in heads_to_reinforce.keys():\n",
    "    #    return\n",
    "    for h in range(heads):\n",
    "        #if h not in heads_to_reinforce[layer]:\n",
    "        #    continue\n",
    "        acc = 0\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            for j in range(seq_len):\n",
    "                if (i, j) in attn_labels_1_idx:\n",
    "                    value[0, h, i, j] += 0.1\n",
    "                    #acc += value[0, h, i, j]\n",
    "                    pass\n",
    "        \n",
    "        #visualize_attention_heads(value, None, title=f\"Example Attention Heatmap Layer {layer}\", heads=[h], length=keep_last_n_tokens, attn_labels=attn_labels)\n",
    "\n",
    "    return value\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "is_attn_scores = lambda name: name.endswith(\"hook_pattern\")\n",
    "ablated = []\n",
    "for idx in range(0, 10):\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    indices =  np.argwhere(attn_labels[idx].cpu().numpy() == 1)\n",
    "    attn_labels_1_idx = set(map(tuple, indices))\n",
    "    \n",
    "\n",
    "    model.to(device)\n",
    "    input_ids[idx] = input_ids[idx].to(device)\n",
    "    target_ids[idx] = target_ids[idx].to(device)\n",
    "    attn_labels[idx] = attn_labels[idx].to(device)\n",
    "                \n",
    "    heads_to_reinforce_x = [\n",
    "        (9, [5, 7, 20, 25, 29, 30, 31]),\n",
    "        (10, [8]),\n",
    "        (11, [9, 8, 30]),\n",
    "        (13, [9, 11]),\n",
    "        (14, [24, 26, 27]),\n",
    "        (15, [1, 8, 10]),\n",
    "\n",
    "    ]\n",
    "\n",
    "    cols_to_keep = ~np.all(attn_labels[idx].cpu().numpy() == -100, axis=0)\n",
    "    keep_last_n_tokens = 70\n",
    "    heads_x = dict(heads_to_reinforce_x)\n",
    "    print(heads_x)\n",
    "    logits = model.run_with_hooks(input_ids[idx], return_type=\"logits\",\n",
    "                                    fwd_hooks=[\n",
    "                                        (is_attn_scores, partial(fix_attn_pattern_hook, attn_labels_1_idx=attn_labels_1_idx, heads_to_reinforce=heads_x, attn_labels=attn_labels[idx], keep_last_n_tokens = keep_last_n_tokens))\n",
    "                                    ])\n",
    "    loss, acc = compute_scores(logits, target_ids[idx])\n",
    "    ablated.append((heads_x, loss, acc))\n",
    "    del logits\n",
    "    torch.cuda.empty_cache()\n",
    "    input_ids[idx] = input_ids[idx].to(\"cpu\")\n",
    "    target_ids[idx] = target_ids[idx].to(\"cpu\")\n",
    "    attn_labels[idx] = attn_labels[idx].to(\"cpu\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n",
      "25.5\n"
     ]
    }
   ],
   "source": [
    "sums = 0\n",
    "for item in ablated:\n",
    "    sums += item[2]\n",
    "print(sums/len(ablated))\n",
    "print(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('blocks.11.attn.hook_pattern', 9),\n",
       "  tensor(29.8965, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 24),\n",
       "  tensor(29.8524, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 8),\n",
       "  tensor(29.3969, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 30),\n",
       "  tensor(29.0661, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 9),\n",
       "  tensor(29.0473, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 29),\n",
       "  tensor(28.1519, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 1),\n",
       "  tensor(26.8898, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 25),\n",
       "  tensor(26.2973, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 8),\n",
       "  tensor(26.1444, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 5),\n",
       "  tensor(24.7707, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 7),\n",
       "  tensor(20.4726, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 27),\n",
       "  tensor(19.9416, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 10),\n",
       "  tensor(16.7819, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 26),\n",
       "  tensor(16.3783, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 30),\n",
       "  tensor(16.3017, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 8),\n",
       "  tensor(14.5482, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 11),\n",
       "  tensor(13.6111, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 31),\n",
       "  tensor(12.0554, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 20),\n",
       "  tensor(11.5404, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 11),\n",
       "  tensor(11.2330, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 10),\n",
       "  tensor(10.7935, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 15),\n",
       "  tensor(10.4889, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 4),\n",
       "  tensor(9.4858, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 13),\n",
       "  tensor(8.8022, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 29),\n",
       "  tensor(8.7715, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 30),\n",
       "  tensor(8.5120, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 22),\n",
       "  tensor(8.4509, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 21),\n",
       "  tensor(8.3930, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 2),\n",
       "  tensor(8.3610, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 12),\n",
       "  tensor(8.3278, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 3),\n",
       "  tensor(8.2413, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 1),\n",
       "  tensor(8.2308, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 31),\n",
       "  tensor(8.2208, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 30),\n",
       "  tensor(8.2183, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 21),\n",
       "  tensor(8.1319, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 28),\n",
       "  tensor(8.0696, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 29),\n",
       "  tensor(8.0194, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 20),\n",
       "  tensor(7.9135, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 20),\n",
       "  tensor(7.7341, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 15),\n",
       "  tensor(7.7331, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 2),\n",
       "  tensor(7.7221, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 11),\n",
       "  tensor(7.5884, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 19),\n",
       "  tensor(7.5706, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 31),\n",
       "  tensor(7.5472, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 29),\n",
       "  tensor(7.4909, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 31),\n",
       "  tensor(7.4321, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 22),\n",
       "  tensor(7.3331, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 14),\n",
       "  tensor(7.2897, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 20),\n",
       "  tensor(7.2405, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 15),\n",
       "  tensor(7.1728, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 14),\n",
       "  tensor(7.1488, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 1),\n",
       "  tensor(7.1361, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 14),\n",
       "  tensor(7.1259, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 21),\n",
       "  tensor(7.1253, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 13),\n",
       "  tensor(7.1233, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 15),\n",
       "  tensor(7.0611, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 30),\n",
       "  tensor(7.0376, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 0),\n",
       "  tensor(6.9761, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 8),\n",
       "  tensor(6.9353, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 23),\n",
       "  tensor(6.8604, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 16),\n",
       "  tensor(6.8200, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 15),\n",
       "  tensor(6.7089, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 17),\n",
       "  tensor(6.6321, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 6),\n",
       "  tensor(6.6049, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 9),\n",
       "  tensor(6.6033, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 25),\n",
       "  tensor(6.5929, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 7),\n",
       "  tensor(6.5838, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 10),\n",
       "  tensor(6.4921, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 27),\n",
       "  tensor(6.4858, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 23),\n",
       "  tensor(6.3761, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 13),\n",
       "  tensor(6.3681, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 12),\n",
       "  tensor(6.2044, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 6),\n",
       "  tensor(6.1921, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 28),\n",
       "  tensor(6.1864, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 5),\n",
       "  tensor(6.1751, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 3),\n",
       "  tensor(6.1501, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 12),\n",
       "  tensor(6.1171, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 27),\n",
       "  tensor(5.7963, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 21),\n",
       "  tensor(5.6334, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 2),\n",
       "  tensor(5.6299, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 14),\n",
       "  tensor(5.5982, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 10),\n",
       "  tensor(5.5296, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 30),\n",
       "  tensor(5.2120, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 0),\n",
       "  tensor(5.1390, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 20),\n",
       "  tensor(4.5321, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 26),\n",
       "  tensor(4.5023, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 23),\n",
       "  tensor(4.3912, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 2),\n",
       "  tensor(4.3830, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 9),\n",
       "  tensor(4.2525, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 4),\n",
       "  tensor(4.2156, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 2),\n",
       "  tensor(4.1924, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 15),\n",
       "  tensor(4.1244, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 18),\n",
       "  tensor(3.9764, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 11),\n",
       "  tensor(3.9243, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 11),\n",
       "  tensor(3.8595, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 6),\n",
       "  tensor(3.5277, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 18),\n",
       "  tensor(3.5075, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 4),\n",
       "  tensor(3.4975, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 18),\n",
       "  tensor(3.4256, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 31),\n",
       "  tensor(3.3841, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 25),\n",
       "  tensor(3.3653, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 10),\n",
       "  tensor(3.3537, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 4),\n",
       "  tensor(3.3502, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 19),\n",
       "  tensor(3.3280, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 17),\n",
       "  tensor(3.3096, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 3),\n",
       "  tensor(3.1354, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 21),\n",
       "  tensor(3.1112, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 12),\n",
       "  tensor(2.9813, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 20),\n",
       "  tensor(2.9091, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 7),\n",
       "  tensor(2.8059, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 19),\n",
       "  tensor(2.7703, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 4),\n",
       "  tensor(2.7557, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 27),\n",
       "  tensor(2.6900, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 29),\n",
       "  tensor(2.5862, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 27),\n",
       "  tensor(2.4480, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 24),\n",
       "  tensor(2.4406, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 22),\n",
       "  tensor(2.3049, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 24),\n",
       "  tensor(2.2425, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 19),\n",
       "  tensor(2.1096, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 21),\n",
       "  tensor(2.0604, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 24),\n",
       "  tensor(2.0483, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 26),\n",
       "  tensor(2.0273, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 28),\n",
       "  tensor(2.0226, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 14),\n",
       "  tensor(1.9844, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 17),\n",
       "  tensor(1.9739, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 3),\n",
       "  tensor(1.9414, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 25),\n",
       "  tensor(1.9358, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 8),\n",
       "  tensor(1.9214, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 0),\n",
       "  tensor(1.9083, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 22),\n",
       "  tensor(1.8800, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 16),\n",
       "  tensor(1.8789, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 22),\n",
       "  tensor(1.7793, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 13),\n",
       "  tensor(1.7151, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 26),\n",
       "  tensor(1.6736, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 9),\n",
       "  tensor(1.6638, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 22),\n",
       "  tensor(1.6189, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 27),\n",
       "  tensor(1.6175, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 6),\n",
       "  tensor(1.5683, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 19),\n",
       "  tensor(1.4822, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 26),\n",
       "  tensor(1.4780, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 25),\n",
       "  tensor(1.4452, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 6),\n",
       "  tensor(1.4392, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 15),\n",
       "  tensor(1.3802, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 5),\n",
       "  tensor(1.3547, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 19),\n",
       "  tensor(1.3085, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 21),\n",
       "  tensor(1.2983, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 24),\n",
       "  tensor(1.2811, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 18),\n",
       "  tensor(1.2770, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 21),\n",
       "  tensor(1.2722, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 19),\n",
       "  tensor(1.2635, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 1),\n",
       "  tensor(1.2626, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 30),\n",
       "  tensor(1.2472, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 3),\n",
       "  tensor(1.2282, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 9),\n",
       "  tensor(1.2234, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 12),\n",
       "  tensor(1.2160, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 3),\n",
       "  tensor(1.2077, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 16),\n",
       "  tensor(1.1987, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 29),\n",
       "  tensor(1.1958, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 30),\n",
       "  tensor(1.1084, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 29),\n",
       "  tensor(1.0887, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 17),\n",
       "  tensor(1.0601, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 3),\n",
       "  tensor(1.0592, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 28),\n",
       "  tensor(1.0550, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 13),\n",
       "  tensor(1.0421, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 7),\n",
       "  tensor(1.0157, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 27),\n",
       "  tensor(1.0072, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 23),\n",
       "  tensor(1.0058, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 27),\n",
       "  tensor(1.0058, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 22),\n",
       "  tensor(1.0048, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 22),\n",
       "  tensor(0.9987, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 1),\n",
       "  tensor(0.9373, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 8),\n",
       "  tensor(0.9171, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 19),\n",
       "  tensor(0.8928, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 4),\n",
       "  tensor(0.8897, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 6),\n",
       "  tensor(0.8890, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 0),\n",
       "  tensor(0.8640, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 2),\n",
       "  tensor(0.8255, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 28),\n",
       "  tensor(0.8250, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 23),\n",
       "  tensor(0.8211, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 17),\n",
       "  tensor(0.8186, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 25),\n",
       "  tensor(0.8141, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 25),\n",
       "  tensor(0.8120, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 5),\n",
       "  tensor(0.8091, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 7),\n",
       "  tensor(0.8014, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 12),\n",
       "  tensor(0.7819, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 16),\n",
       "  tensor(0.7812, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 5),\n",
       "  tensor(0.7669, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 23),\n",
       "  tensor(0.7468, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 24),\n",
       "  tensor(0.7268, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 0),\n",
       "  tensor(0.7221, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 14),\n",
       "  tensor(0.7177, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 3),\n",
       "  tensor(0.7027, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 25),\n",
       "  tensor(0.6800, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 2),\n",
       "  tensor(0.6784, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 24),\n",
       "  tensor(0.6613, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 15),\n",
       "  tensor(0.6037, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 5),\n",
       "  tensor(0.5912, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 31),\n",
       "  tensor(0.5791, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 17),\n",
       "  tensor(0.5739, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 27),\n",
       "  tensor(0.5697, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 11),\n",
       "  tensor(0.5649, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 13),\n",
       "  tensor(0.5510, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 10),\n",
       "  tensor(0.5476, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 4),\n",
       "  tensor(0.5467, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 7),\n",
       "  tensor(0.5433, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 9),\n",
       "  tensor(0.5364, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 23),\n",
       "  tensor(0.5292, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 1),\n",
       "  tensor(0.5158, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 8),\n",
       "  tensor(0.5118, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 10),\n",
       "  tensor(0.5104, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 14),\n",
       "  tensor(0.5071, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 5),\n",
       "  tensor(0.5051, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 16),\n",
       "  tensor(0.4926, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 8),\n",
       "  tensor(0.4917, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 1),\n",
       "  tensor(0.4803, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 10),\n",
       "  tensor(0.4749, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 13),\n",
       "  tensor(0.4748, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 28),\n",
       "  tensor(0.4635, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 24),\n",
       "  tensor(0.4626, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 15),\n",
       "  tensor(0.4587, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 1),\n",
       "  tensor(0.4566, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 7),\n",
       "  tensor(0.4562, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 20),\n",
       "  tensor(0.4544, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 0),\n",
       "  tensor(0.4455, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 13),\n",
       "  tensor(0.4271, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 0),\n",
       "  tensor(0.4262, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 2),\n",
       "  tensor(0.4228, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 1),\n",
       "  tensor(0.4158, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 31),\n",
       "  tensor(0.4104, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 5),\n",
       "  tensor(0.4010, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 26),\n",
       "  tensor(0.4008, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 13),\n",
       "  tensor(0.3962, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 21),\n",
       "  tensor(0.3959, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 20),\n",
       "  tensor(0.3935, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 24),\n",
       "  tensor(0.3835, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 28),\n",
       "  tensor(0.3821, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 14),\n",
       "  tensor(0.3730, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 16),\n",
       "  tensor(0.3716, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 26),\n",
       "  tensor(0.3647, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 29),\n",
       "  tensor(0.3641, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 5),\n",
       "  tensor(0.3585, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 19),\n",
       "  tensor(0.3509, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 12),\n",
       "  tensor(0.3500, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 1),\n",
       "  tensor(0.3441, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 26),\n",
       "  tensor(0.3426, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 15),\n",
       "  tensor(0.3424, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 24),\n",
       "  tensor(0.3377, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 25),\n",
       "  tensor(0.3365, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 27),\n",
       "  tensor(0.3288, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 31),\n",
       "  tensor(0.3261, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 15),\n",
       "  tensor(0.3235, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 7),\n",
       "  tensor(0.3229, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 0),\n",
       "  tensor(0.3057, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 11),\n",
       "  tensor(0.3026, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 24),\n",
       "  tensor(0.3001, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 8),\n",
       "  tensor(0.2984, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 15),\n",
       "  tensor(0.2978, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 22),\n",
       "  tensor(0.2940, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 17),\n",
       "  tensor(0.2901, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 6),\n",
       "  tensor(0.2868, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 17),\n",
       "  tensor(0.2857, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 28),\n",
       "  tensor(0.2856, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 25),\n",
       "  tensor(0.2829, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 18),\n",
       "  tensor(0.2794, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 12),\n",
       "  tensor(0.2782, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 17),\n",
       "  tensor(0.2759, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 25),\n",
       "  tensor(0.2755, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 13),\n",
       "  tensor(0.2730, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 23),\n",
       "  tensor(0.2669, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 26),\n",
       "  tensor(0.2669, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 18),\n",
       "  tensor(0.2661, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 3),\n",
       "  tensor(0.2646, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 0),\n",
       "  tensor(0.2632, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 4),\n",
       "  tensor(0.2625, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 25),\n",
       "  tensor(0.2623, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 28),\n",
       "  tensor(0.2619, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 27),\n",
       "  tensor(0.2617, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 21),\n",
       "  tensor(0.2579, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 17),\n",
       "  tensor(0.2518, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 26),\n",
       "  tensor(0.2504, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 28),\n",
       "  tensor(0.2466, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 29),\n",
       "  tensor(0.2464, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 3),\n",
       "  tensor(0.2454, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 4),\n",
       "  tensor(0.2436, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 16),\n",
       "  tensor(0.2403, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 13),\n",
       "  tensor(0.2361, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 10),\n",
       "  tensor(0.2353, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 28),\n",
       "  tensor(0.2328, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 11),\n",
       "  tensor(0.2272, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 23),\n",
       "  tensor(0.2267, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 2),\n",
       "  tensor(0.2253, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 30),\n",
       "  tensor(0.2222, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.10.attn.hook_pattern', 6),\n",
       "  tensor(0.2127, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 0),\n",
       "  tensor(0.2095, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 9),\n",
       "  tensor(0.2016, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 0),\n",
       "  tensor(0.2000, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 30),\n",
       "  tensor(0.1992, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 2),\n",
       "  tensor(0.1987, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 6),\n",
       "  tensor(0.1976, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 16),\n",
       "  tensor(0.1940, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 14),\n",
       "  tensor(0.1918, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 20),\n",
       "  tensor(0.1916, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 10),\n",
       "  tensor(0.1815, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 18),\n",
       "  tensor(0.1805, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 17),\n",
       "  tensor(0.1780, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 29),\n",
       "  tensor(0.1758, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 5),\n",
       "  tensor(0.1758, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 20),\n",
       "  tensor(0.1747, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 3),\n",
       "  tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 22),\n",
       "  tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 3),\n",
       "  tensor(0.1712, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 21),\n",
       "  tensor(0.1702, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 9),\n",
       "  tensor(0.1637, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 16),\n",
       "  tensor(0.1571, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 29),\n",
       "  tensor(0.1546, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 6),\n",
       "  tensor(0.1525, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 13),\n",
       "  tensor(0.1522, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 19),\n",
       "  tensor(0.1513, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 28),\n",
       "  tensor(0.1513, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 31),\n",
       "  tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 10),\n",
       "  tensor(0.1480, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 5),\n",
       "  tensor(0.1471, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 15),\n",
       "  tensor(0.1433, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 3),\n",
       "  tensor(0.1401, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 23),\n",
       "  tensor(0.1386, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 16),\n",
       "  tensor(0.1375, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 22),\n",
       "  tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 16),\n",
       "  tensor(0.1355, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 13),\n",
       "  tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 14),\n",
       "  tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 31),\n",
       "  tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 31),\n",
       "  tensor(0.1329, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 14),\n",
       "  tensor(0.1325, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 4),\n",
       "  tensor(0.1317, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 19),\n",
       "  tensor(0.1277, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 9),\n",
       "  tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 12),\n",
       "  tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 18),\n",
       "  tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 26),\n",
       "  tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 8),\n",
       "  tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 6),\n",
       "  tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 1),\n",
       "  tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 11),\n",
       "  tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 7),\n",
       "  tensor(0.1183, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.15.attn.hook_pattern', 18),\n",
       "  tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 7),\n",
       "  tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 24),\n",
       "  tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 23),\n",
       "  tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 7),\n",
       "  tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 3),\n",
       "  tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 7),\n",
       "  tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 19),\n",
       "  tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 19),\n",
       "  tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 7),\n",
       "  tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 2),\n",
       "  tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 20),\n",
       "  tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 8),\n",
       "  tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 10),\n",
       "  tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 31),\n",
       "  tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 5),\n",
       "  tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 18),\n",
       "  tensor(0.0983, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 22),\n",
       "  tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 20),\n",
       "  tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 10),\n",
       "  tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 9),\n",
       "  tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 27),\n",
       "  tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 18),\n",
       "  tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 12),\n",
       "  tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 25),\n",
       "  tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 23),\n",
       "  tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 27),\n",
       "  tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 7),\n",
       "  tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 18),\n",
       "  tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 27),\n",
       "  tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 0),\n",
       "  tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 29),\n",
       "  tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 11),\n",
       "  tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 12),\n",
       "  tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 31),\n",
       "  tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 16),\n",
       "  tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 6),\n",
       "  tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 9),\n",
       "  tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 14),\n",
       "  tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 8),\n",
       "  tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 5),\n",
       "  tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 7),\n",
       "  tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 21),\n",
       "  tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 6),\n",
       "  tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 28),\n",
       "  tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 11),\n",
       "  tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 4),\n",
       "  tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 10),\n",
       "  tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 21),\n",
       "  tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 5),\n",
       "  tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 5),\n",
       "  tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 26),\n",
       "  tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 2),\n",
       "  tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 11),\n",
       "  tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 20),\n",
       "  tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 4),\n",
       "  tensor(0.0569, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 16),\n",
       "  tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 15),\n",
       "  tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 9),\n",
       "  tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 1),\n",
       "  tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 19),\n",
       "  tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 17),\n",
       "  tensor(0.0512, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 17),\n",
       "  tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 22),\n",
       "  tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 30),\n",
       "  tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 21),\n",
       "  tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 23),\n",
       "  tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 4),\n",
       "  tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 7),\n",
       "  tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 12),\n",
       "  tensor(0.0409, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 22),\n",
       "  tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 30),\n",
       "  tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 29),\n",
       "  tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 20),\n",
       "  tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 14),\n",
       "  tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 12),\n",
       "  tensor(0.0355, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 6),\n",
       "  tensor(0.0348, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 8),\n",
       "  tensor(0.0342, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.14.attn.hook_pattern', 23),\n",
       "  tensor(0.0317, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 30),\n",
       "  tensor(0.0306, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 30),\n",
       "  tensor(0.0290, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 14),\n",
       "  tensor(0.0283, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 31),\n",
       "  tensor(0.0282, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 16),\n",
       "  tensor(0.0282, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 17),\n",
       "  tensor(0.0277, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 8),\n",
       "  tensor(0.0275, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 6),\n",
       "  tensor(0.0270, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 13),\n",
       "  tensor(0.0269, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 24),\n",
       "  tensor(0.0269, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 0),\n",
       "  tensor(0.0267, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 30),\n",
       "  tensor(0.0253, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 24),\n",
       "  tensor(0.0251, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 6),\n",
       "  tensor(0.0251, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.11.attn.hook_pattern', 0),\n",
       "  tensor(0.0249, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 14),\n",
       "  tensor(0.0246, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.13.attn.hook_pattern', 26),\n",
       "  tensor(0.0244, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 2),\n",
       "  tensor(0.0241, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 4),\n",
       "  tensor(0.0241, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 20),\n",
       "  tensor(0.0240, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 0),\n",
       "  tensor(0.0237, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 21),\n",
       "  tensor(0.0225, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 9),\n",
       "  tensor(0.0221, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 8),\n",
       "  tensor(0.0219, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 3),\n",
       "  tensor(0.0218, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 4),\n",
       "  tensor(0.0204, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 15),\n",
       "  tensor(0.0202, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 1),\n",
       "  tensor(0.0196, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 13),\n",
       "  tensor(0.0196, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 26),\n",
       "  tensor(0.0189, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 21),\n",
       "  tensor(0.0188, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.7.attn.hook_pattern', 12),\n",
       "  tensor(0.0187, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 25),\n",
       "  tensor(0.0186, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 29),\n",
       "  tensor(0.0185, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 2),\n",
       "  tensor(0.0180, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 24),\n",
       "  tensor(0.0175, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 25),\n",
       "  tensor(0.0170, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 3),\n",
       "  tensor(0.0155, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 28),\n",
       "  tensor(0.0150, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 23),\n",
       "  tensor(0.0148, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 2),\n",
       "  tensor(0.0139, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 1),\n",
       "  tensor(0.0136, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 17),\n",
       "  tensor(0.0127, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 28),\n",
       "  tensor(0.0124, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 18),\n",
       "  tensor(0.0124, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 9),\n",
       "  tensor(0.0123, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 18),\n",
       "  tensor(0.0116, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 12),\n",
       "  tensor(0.0107, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 27),\n",
       "  tensor(0.0106, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 26),\n",
       "  tensor(0.0102, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.12.attn.hook_pattern', 11),\n",
       "  tensor(0.0101, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 10),\n",
       "  tensor(0.0101, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.6.attn.hook_pattern', 11),\n",
       "  tensor(0.0098, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 19),\n",
       "  tensor(0.0096, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 20),\n",
       "  tensor(0.0094, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 27),\n",
       "  tensor(0.0091, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 28),\n",
       "  tensor(0.0090, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 18),\n",
       "  tensor(0.0088, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 17),\n",
       "  tensor(0.0088, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 4),\n",
       "  tensor(0.0087, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 1),\n",
       "  tensor(0.0085, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 24),\n",
       "  tensor(0.0082, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 1),\n",
       "  tensor(0.0080, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 26),\n",
       "  tensor(0.0075, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 2),\n",
       "  tensor(0.0073, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 11),\n",
       "  tensor(0.0073, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 31),\n",
       "  tensor(0.0065, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 22),\n",
       "  tensor(0.0063, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 9),\n",
       "  tensor(0.0061, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 25),\n",
       "  tensor(0.0052, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 16),\n",
       "  tensor(0.0052, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 10),\n",
       "  tensor(0.0050, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 22),\n",
       "  tensor(0.0044, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 18),\n",
       "  tensor(0.0044, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 12),\n",
       "  tensor(0.0041, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 15),\n",
       "  tensor(0.0033, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.4.attn.hook_pattern', 29),\n",
       "  tensor(0.0032, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 0),\n",
       "  tensor(0.0031, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 16),\n",
       "  tensor(0.0031, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.3.attn.hook_pattern', 13),\n",
       "  tensor(0.0029, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 5),\n",
       "  tensor(0.0025, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.9.attn.hook_pattern', 8),\n",
       "  tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 30),\n",
       "  tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.1.attn.hook_pattern', 23),\n",
       "  tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 29),\n",
       "  tensor(0.0008, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.0.attn.hook_pattern', 31),\n",
       "  tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.2.attn.hook_pattern', 14),\n",
       "  tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.8.attn.hook_pattern', 19),\n",
       "  tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)),\n",
       " (('blocks.5.attn.hook_pattern', 11),\n",
       "  tensor(7.3986e-05, device='cuda:0', grad_fn=<AddBackward0>))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(best_heads.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
