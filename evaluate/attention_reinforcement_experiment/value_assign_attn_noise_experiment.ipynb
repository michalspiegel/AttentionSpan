{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipykernel transformer_lens plotly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens\n",
    "import torch\n",
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-1.5B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Load a model (eg GPT-2 Small)\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "# Run the model and get logits and activations\n",
    "logits, activations = model.run_with_cache(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert HF Transformers weights to HookedTransformer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.pretrained.weight_conversions.llama import convert_llama_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "model.load_state_dict(torch.load(\"checkpoints/old/llama_instruct_value_assign_finetuned.weights\", weights_only=True))\n",
    "hooked_config = transformer_lens.loading.get_pretrained_model_config(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "new_state_dict = convert_llama_weights(model, hooked_config)\n",
    "torch.save(new_state_dict, \"checkpoints/old/llama_instruct_value_assign_finetuned_hooked.weights\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    }
   ],
   "source": [
    "config = transformer_lens.loading.get_pretrained_model_config(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = transformer_lens.HookedTransformer(config)\n",
    "model.load_and_process_state_dict(torch.load(\"checkpoints/old/llama_instruct_value_assign_finetuned_hooked.weights\", weights_only=True))\n",
    "#config = transformer_lens.loading.get_pretrained_model_config(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "#model = transformer_lens.HookedTransformer(config)\n",
    "#model.load_and_process_state_dict(torch.load(\"checkpoints/Qwen2.5-1.5B-Instruct_value_assign_finetuned_hooked.weights\", weights_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "sys.path.insert(0, \"../generators\")\n",
    "sys.path.insert(0, \"generators\")\n",
    "\n",
    "\n",
    "from generators.value_assignment import ValueAssignmentGenerator\n",
    "import transformers\n",
    "import itertools\n",
    "\n",
    "\n",
    "data_generator = ValueAssignmentGenerator(tokenizer=model.tokenizer,\n",
    "                                       seed=0,\n",
    "                                       use_few_shot=True, \n",
    "                                       use_instruction=True, \n",
    "                                       apply_chat_template=True,\n",
    "                                       length=(5,5),\n",
    "                                       encoding_length=(5,5)\n",
    "                                       )\n",
    "samples = itertools.islice(data_generator.generate_samples(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(samples)\n",
    "texts = [sample[0] for sample in samples]\n",
    "input_ids = [sample[1] for sample in samples]\n",
    "target_ids = [sample[2] for sample in samples]\n",
    "attn_labels = [sample[3] for sample in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 16 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\\nYou are a great problem solver excellent in tasks requiring manipulation of symbols. You will be tasked with encoding a sequence of characters given a set of encodings. \\nAn encoding is a tuple of characters, e.g. A1, where A is the character to be encoded, and 1 is its encoding. For example, given some string \"A\", its encoding is a sequence \"1\". \\nGiven a set of these encodings and an input string, encode this string.\\nDo not generate any words. You can only generate characters from the set of encodings.\\nHere are some examples:\\nH3K8U1D5G9F1 HKKHDFG=3883519\\nABCDEFGHIJKL ACEGIK=BDFHJLUse them to solve the following task:\\n\\x04_WT&\\x06CGc^ &c&WC=<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\\x06^\\x06TG<|eot_id|>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def visualize_attention_heads(attention_matrix, tokens=None, title=\"Attention Heatmap\", save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize a multi-head attention matrix as heatmaps for each head.\n",
    "\n",
    "    Parameters:\n",
    "        attention_matrix (torch.Tensor): Tensor of shape (1, num_heads, seq_len, seq_len).\n",
    "        tokens (list, optional): List of tokens corresponding to the rows/columns of the matrix.\n",
    "        title (str): Title of the heatmap.\n",
    "        save_path (str, optional): Path to save the heatmap images. If None, the heatmaps are not saved.\n",
    "    \"\"\"\n",
    "    # Detach and convert to NumPy\n",
    "    scores = attention_matrix.squeeze(0).cpu().detach().numpy()  # Shape: (num_heads, seq_len, seq_len)\n",
    "    num_heads = scores.shape[0]\n",
    "    for head in range(num_heads):\n",
    "        head_scores = scores[head]  # Shape: (seq_len, seq_len)\n",
    "        min_score = head_scores.min()\n",
    "        max_score = head_scores.max()\n",
    "        normalized = (head_scores - min_score) / (max_score - min_score)  # Normalize to [0, 1]\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(normalized, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "        plt.colorbar(label=\"Attention Score\")\n",
    "        plt.title(f\"{title} - Head {head + 1}\")\n",
    "        \n",
    "        if tokens:\n",
    "            plt.xticks(ticks=np.arange(len(tokens)), labels=tokens, rotation=90)\n",
    "            plt.yticks(ticks=np.arange(len(tokens)), labels=tokens)\n",
    "        else:\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "        plt.xlabel(\"Tokens\")\n",
    "        plt.ylabel(\"Tokens\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(f\"{save_path}_head_{head + 1}.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "Running ablated with noise 0.0\n",
      "Loss: 0.00016773004608694464, Accuracy: 100.00%\n",
      "Running ablated with noise 0.00202020202020202\n",
      "Loss: 0.047034427523612976, Accuracy: 100.00%\n",
      "Running ablated with noise 0.00404040404040404\n",
      "Loss: 2.879239082336426, Accuracy: 50.00%\n",
      "Running ablated with noise 0.006060606060606061\n",
      "Loss: 4.704164028167725, Accuracy: 16.67%\n",
      "Running ablated with noise 0.00808080808080808\n",
      "Loss: 5.344171047210693, Accuracy: 16.67%\n",
      "Running ablated with noise 0.0101010101010101\n",
      "Loss: 5.738537311553955, Accuracy: 16.67%\n",
      "Running ablated with noise 0.012121212121212121\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning ablated with noise\u001b[39m\u001b[38;5;124m\"\u001b[39m, noise_level)\n\u001b[1;32m     70\u001b[0m cols_to_keep \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39mall(attn_labels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfwd_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_attn_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_noise_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcols_to_keep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                              \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m ablated_scores\u001b[38;5;241m.\u001b[39mappend(compute_scores(logits, target_ids[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m logits\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/transformer_lens/hook_points.py:456\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_hooks\u001b[0;34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts) \u001b[38;5;28;01mas\u001b[39;00m hooked_model:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhooked_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:612\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    609\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    610\u001b[0m         )\n\u001b[0;32m--> 612\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/transformer_lens/components/transformer_block.py:160\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    153\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    154\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    156\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_post(attn_out)\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/transformer_lens/components/abstract_attention.py:261\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    259\u001b[0m pattern \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attn_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    260\u001b[0m pattern \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(torch\u001b[38;5;241m.\u001b[39misnan(pattern), torch\u001b[38;5;241m.\u001b[39mzeros_like(pattern), pattern)\n\u001b[0;32m--> 261\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_pattern\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, head_index, query_pos, key_pos]\u001b[39;00m\n\u001b[1;32m    262\u001b[0m pattern \u001b[38;5;241m=\u001b[39m pattern\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    263\u001b[0m pattern \u001b[38;5;241m=\u001b[39m pattern\u001b[38;5;241m.\u001b[39mto(v\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1806\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1804\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1806\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1809\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/transformer_lens/hook_points.py:109\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbwd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m ):  \u001b[38;5;66;03m# For a backwards hook, module_output is a tuple of (grad,) - I don't know why.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     module_output \u001b[38;5;241m=\u001b[39m module_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 53\u001b[0m, in \u001b[0;36mattn_noise_hook\u001b[0;34m(value, hook, noise_level, sample_length)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mattn_noise_hook\u001b[39m(\n\u001b[1;32m     48\u001b[0m     value: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos head_index d_head\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     49\u001b[0m     hook: HookPoint,\n\u001b[1;32m     50\u001b[0m     noise_level: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m     51\u001b[0m     sample_length\n\u001b[1;32m     52\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos head_index d_head\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 53\u001b[0m     noise \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     value[:, :, :, :] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(noise, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m#visualize_attention_heads(value, None, title=\"Example Attention Heatmap\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 43\u001b[0m, in \u001b[0;36mgenerate_noise\u001b[0;34m(value_shape, noise_level, sample_length)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):  \u001b[38;5;66;03m# Lower triangle condition (j < i)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m seq_len \u001b[38;5;241m-\u001b[39m sample_length:  \u001b[38;5;66;03m# Ensure it's within the sample tokens\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m                 noise[\u001b[38;5;241m0\u001b[39m, h, i, j] \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m noise_level\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m noise\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global noise\n",
    "def compute_scores(logits, target):\n",
    "    ignore_index = -100  # Optional: index to ignore in loss/accuracy computation\n",
    "    # Compute loss\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "    logits.view(-1, logits.size(-1)),  # Flatten logits to (batch_size * seq_len, vocab_size)\n",
    "    target.view(-1),               # Flatten target_ids to (batch_size * seq_len)\n",
    "    ignore_index=ignore_index          # Ignore padding tokens if applicable\n",
    ")\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = torch.argmax(logits, dim=-1)  # Get predicted token indices\n",
    "    correct = (predictions == target) & (target != ignore_index)  # Ignore padding tokens\n",
    "    accuracy = correct.sum().item() / (target != ignore_index).sum().item()\n",
    "\n",
    "    print(f\"Loss: {loss.item()}, Accuracy: {accuracy:.2%}\")\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "def generate_noise(value_shape, noise_level, sample_length):\n",
    "    \"\"\"\n",
    "    Generate noise only in the lower triangle of the attention matrix for the last n tokens,\n",
    "    where n is the number of tokens in the target_ids that are not -100.\n",
    "\n",
    "    Parameters:\n",
    "        value_shape (tuple): Shape of the attention matrix (batch, seq_len, head_index, d_head).\n",
    "        noise_level (float): Level of noise to apply.\n",
    "        target_ids (torch.Tensor): Target IDs tensor to determine the length of the sample.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Noise matrix with noise only in the lower triangle for the sample tokens.\n",
    "    \"\"\"\n",
    "    batch_size, heads, seq_len, _ = value_shape\n",
    "    global noise\n",
    "    # Determine the length of the sample (number of tokens not equal to -100 in target_ids)\n",
    "    # Initialize noise matrix\n",
    "    noise = np.zeros(value_shape)\n",
    "\n",
    "    for h in range(heads):\n",
    "        # Generate noise only for the lower triangle of the sample tokens\n",
    "        for i in range(seq_len):\n",
    "            for j in range(i+1):  # Lower triangle condition (j < i)\n",
    "                if j >= seq_len - sample_length:  # Ensure it's within the sample tokens\n",
    "                    noise[0, h, i, j] = random.random() * noise_level\n",
    "\n",
    "    return noise\n",
    "\n",
    "def attn_noise_hook(\n",
    "    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint,\n",
    "    noise_level: float,\n",
    "    sample_length\n",
    ") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
    "    noise = generate_noise(value.shape, noise_level, sample_length)\n",
    "    value[:, :, :, :] += torch.tensor(noise, dtype=torch.float32).cuda()\n",
    "    #visualize_attention_heads(value, None, title=\"Example Attention Heatmap\")\n",
    "    return value\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "is_attn_scores = lambda name: name.endswith(\"hook_pattern\")\n",
    "\n",
    "model.to(device)\n",
    "input_ids[0] = input_ids[0].to(device)\n",
    "target_ids[0] = target_ids[0].to(device)\n",
    "\n",
    "ablated_scores = []\n",
    "\n",
    "for noise_level in np.linspace(0, 0.2, 100):\n",
    "    print(\"Running ablated with noise\", noise_level)\n",
    "    cols_to_keep = ~np.all(attn_labels[0].cpu().numpy() == -100, axis=0)\n",
    "    logits = model.run_with_hooks(input_ids[0], return_type=\"logits\",\n",
    "                                  fwd_hooks=[\n",
    "                                      (is_attn_scores, partial(attn_noise_hook, noise_level=noise_level, sample_length=len(cols_to_keep)))\n",
    "                                  ])\n",
    "    ablated_scores.append(compute_scores(logits, target_ids[0]))\n",
    "    del logits\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(noise)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02097557, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00995123, 0.00530414, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00084715, 0.00436938, 0.00925099, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01118269, 0.00343156, 0.0101226 , 0.01672078, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01655927, 0.00339618, 0.00810675, 0.01343579, 0.01269404,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.02068804, 0.01940099, 0.00881088, 0.01038421, 0.01721956,\n",
       "        0.00183712, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01572646, 0.02145712, 0.01930637, 0.00469242, 0.01205831,\n",
       "        0.00658241, 0.01942488, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01995362, 0.00695519, 0.01113861, 0.00510339, 0.00157396,\n",
       "        0.01282777, 0.00670709, 0.02115862, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.0160198 , 0.01800164, 0.0066611 , 0.00478849, 0.0075083 ,\n",
       "        0.00457352, 0.0080598 , 0.00710068, 0.02070562, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.0140865 , 0.01603716, 0.00636359, 0.01845222, 0.00411835,\n",
       "        0.01897187, 0.00371045, 0.02015106, 0.01816933, 0.00616176,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.02210895, 0.02212651, 0.00618412, 0.00952849, 0.02097838,\n",
       "        0.01759283, 0.00829787, 0.00575181, 0.00578472, 0.02211199,\n",
       "        0.00170769, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.02087252, 0.00524475, 0.00281893, 0.00343231, 0.00644935,\n",
       "        0.01194894, 0.00831002, 0.00298608, 0.0212759 , 0.00582641,\n",
       "        0.02208169, 0.01023581, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01704065, 0.00657728, 0.00167747, 0.00769868, 0.00474263,\n",
       "        0.0133969 , 0.01101353, 0.00540332, 0.01374606, 0.01167065,\n",
       "        0.01565461, 0.00953483, 0.02072191, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00766183, 0.01049608, 0.00313764, 0.01463923, 0.0055629 ,\n",
       "        0.00733623, 0.00961793, 0.00944378, 0.01647897, 0.01247511,\n",
       "        0.01717847, 0.00782831, 0.00060624, 0.01716125, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01449474, 0.00168393, 0.00012868, 0.01967157, 0.00177002,\n",
       "        0.01863616, 0.00689968, 0.00720368, 0.01865787, 0.00542922,\n",
       "        0.01860527, 0.00112799, 0.00524421, 0.00197462, 0.00593323,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00474343, 0.01867061, 0.00747563, 0.01632759, 0.01816197,\n",
       "        0.01552174, 0.0004037 , 0.00736052, 0.01139509, 0.00897543,\n",
       "        0.02062028, 0.00508961, 0.01902073, 0.01706911, 0.01002355,\n",
       "        0.00042539, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00497948, 0.00480224, 0.02178668, 0.01997449, 0.00134901,\n",
       "        0.01224199, 0.00684655, 0.02043406, 0.01886346, 0.01839482,\n",
       "        0.00935499, 0.01076427, 0.00441322, 0.00116625, 0.01681152,\n",
       "        0.01163854, 0.02026292, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00556145, 0.02215495, 0.00445181, 0.00525797, 0.01569975,\n",
       "        0.00757096, 0.0047329 , 0.00067126, 0.0008497 , 0.00520489,\n",
       "        0.01785607, 0.0080164 , 0.00776068, 0.01378457, 0.00492719,\n",
       "        0.00708509, 0.01143373, 0.00619312, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01144918, 0.01223946, 0.01616575, 0.01205812, 0.01587511,\n",
       "        0.01124794, 0.01988188, 0.00990503, 0.01434408, 0.00175056,\n",
       "        0.00089614, 0.0124222 , 0.01906912, 0.02178597, 0.02017039,\n",
       "        0.02209003, 0.00816617, 0.00420021, 0.00557926, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01829572, 0.01974308, 0.00650455, 0.01492007, 0.01343858,\n",
       "        0.01664702, 0.01351366, 0.0067692 , 0.00136769, 0.01548465,\n",
       "        0.01073052, 0.01847355, 0.01457365, 0.01111032, 0.01225283,\n",
       "        0.01286919, 0.01813807, 0.01357514, 0.00079697, 0.0049303 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.0052719 , 0.01689712, 0.01685861, 0.01869346, 0.00700792,\n",
       "        0.02126675, 0.01933811, 0.00246911, 0.01603172, 0.0199578 ,\n",
       "        0.0118102 , 0.00614199, 0.02001496, 0.0147358 , 0.01520297,\n",
       "        0.01364322, 0.00232416, 0.01115863, 0.00711178, 0.00256214,\n",
       "        0.00978814, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00382686, 0.02190325, 0.01483334, 0.01747628, 0.0066727 ,\n",
       "        0.01545057, 0.02012668, 0.00930048, 0.00476187, 0.01918926,\n",
       "        0.01075626, 0.00564248, 0.00059756, 0.00371467, 0.00521316,\n",
       "        0.01402354, 0.01857676, 0.02201807, 0.00338777, 0.01927442,\n",
       "        0.00435607, 0.00085796, 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00972501, 0.01301713, 0.00819075, 0.00286602, 0.00859356,\n",
       "        0.01398421, 0.00817409, 0.01855028, 0.02033828, 0.00783543,\n",
       "        0.00505542, 0.01273394, 0.00761857, 0.00456178, 0.01539693,\n",
       "        0.01948637, 0.0112821 , 0.0071478 , 0.01872093, 0.00545639,\n",
       "        0.00284768, 0.01460774, 0.02126825, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00214459, 0.00427531, 0.02089563, 0.01395048, 0.0219353 ,\n",
       "        0.00621101, 0.01503665, 0.00512683, 0.01541735, 0.01772263,\n",
       "        0.01056724, 0.00308626, 0.00318654, 0.01511566, 0.01181051,\n",
       "        0.00084   , 0.01169115, 0.00790089, 0.01854861, 0.01098061,\n",
       "        0.01927573, 0.01445898, 0.01790987, 0.01453657, 0.        ,\n",
       "        0.        ],\n",
       "       [0.01180192, 0.0059572 , 0.01673059, 0.00569589, 0.0087605 ,\n",
       "        0.00111467, 0.00285533, 0.00504707, 0.01693294, 0.01313321,\n",
       "        0.00554499, 0.0125177 , 0.01770997, 0.01967631, 0.015437  ,\n",
       "        0.02188138, 0.00531042, 0.0012063 , 0.00568717, 0.00729245,\n",
       "        0.0108764 , 0.01681558, 0.00329329, 0.00697554, 0.00606193,\n",
       "        0.        ],\n",
       "       [0.02065683, 0.01064006, 0.020152  , 0.0021505 , 0.00158457,\n",
       "        0.01253081, 0.01071116, 0.0189676 , 0.00956427, 0.01927674,\n",
       "        0.0007499 , 0.01424482, 0.00929187, 0.01926782, 0.01421232,\n",
       "        0.01958698, 0.0084262 , 0.01131188, 0.01322251, 0.01345317,\n",
       "        0.01721479, 0.0184323 , 0.01673099, 0.01616924, 0.01478668,\n",
       "        0.01237023]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise[0, 0, 210:, 210:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
