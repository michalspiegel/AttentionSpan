{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (6.29.5)\n",
      "Requirement already satisfied: transformer_lens in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: plotly in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (6.0.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (8.36.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (6.4.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (1.8.14)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: packaging in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (24.2)\n",
      "Requirement already satisfied: psutil in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (6.1.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: pyzmq>=24 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (26.4.0)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: nest-asyncio in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (0.3.2)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: numpy>=1.24 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (4.67.1)\n",
      "Requirement already satisfied: transformers-stream-generator<0.0.6,>=0.0.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (0.0.5)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (3.5.1)\n",
      "Requirement already satisfied: sentencepiece in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (0.2.0)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (1.6.0)\n",
      "Requirement already satisfied: einops>=0.6.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (0.8.1)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (0.14.1)\n",
      "Requirement already satisfied: typeguard<5.0,>=4.2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (4.4.2)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (2.2.3)\n",
      "Requirement already satisfied: rich>=12.6.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (14.0.0)\n",
      "Requirement already satisfied: typing-extensions in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (4.12.2)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (0.19.6)\n",
      "Requirement already satisfied: torch>=2.2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (2.6.0)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: transformers>=4.43 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformer_lens) (4.50.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from plotly) (1.38.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (0.28.1)\n",
      "Requirement already satisfied: pyyaml in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (0.5.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (20.0.0)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (2025.2.0)\n",
      "Requirement already satisfied: filelock in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (3.17.0)\n",
      "Requirement already satisfied: xxhash in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (3.11.18)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
      "Requirement already satisfied: jedi>=0.16 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: exceptiongroup in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (1.2.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (2.19.1)\n",
      "Requirement already satisfied: decorator in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: stack_data in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from jaxtyping>=0.2.11->transformer_lens) (0.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.3.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from pandas>=1.1.5->transformer_lens) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from pandas>=1.1.5->transformer_lens) (2025.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
      "Requirement already satisfied: triton==3.2.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (3.2.0)\n",
      "Requirement already satisfied: jinja2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (3.1.5)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (0.6.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (9.1.0.70)\n",
      "Requirement already satisfied: networkx in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.2->transformer_lens) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformers>=4.43->transformer_lens) (0.21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from transformers>=4.43->transformer_lens) (2024.11.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (5.29.3)\n",
      "Requirement already satisfied: setuptools in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (59.6.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (8.1.8)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (2.21.0)\n",
      "Requirement already satisfied: setproctitle in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (1.3.4)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (2.10.6)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (3.1.44)\n",
      "Requirement already satisfied: six>=1.4.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.17.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (0.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (25.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (5.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.20.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.4.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.6.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (2.27.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from jinja2->torch>=2.2->transformer_lens) (3.0.2)\n",
      "Requirement already satisfied: pure-eval in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (2.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /mnt/silenos/silenos1/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipykernel transformer_lens plotly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=7\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens\n",
    "import torch\n",
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert HF Transformers weights to HookedTransformer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformer_lens.pretrained.weight_conversions.llama import convert_llama_weights\\nfrom transformers import AutoConfig, AutoModelForCausalLM\\n\\nconfig = AutoConfig.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\\nmodel = AutoModelForCausalLM.from_config(config)\\nmodel.load_state_dict(torch.load(\"checkpoints/llama_instruct_value_assign_finetuned_new.weights\", weights_only=True))\\nhooked_config = transformer_lens.loading.get_pretrained_model_config(\"meta-llama/Llama-3.2-1B-Instruct\")\\nnew_state_dict = convert_llama_weights(model, hooked_config)\\ntorch.save(new_state_dict, \"checkpoints/llama_instruct_value_assign_finetuned_hooked_new.weights\")\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformer_lens.pretrained.weight_conversions.llama import convert_llama_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "model.load_state_dict(torch.load(\"checkpoints/llama_instruct_value_assign_finetuned_new.weights\", weights_only=True))\n",
    "hooked_config = transformer_lens.loading.get_pretrained_model_config(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "new_state_dict = convert_llama_weights(model, hooked_config)\n",
    "torch.save(new_state_dict, \"checkpoints/llama_instruct_value_assign_finetuned_hooked_new.weights\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    }
   ],
   "source": [
    "config = transformer_lens.loading.get_pretrained_model_config(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = transformer_lens.HookedTransformer(config)\n",
    "model.load_and_process_state_dict(torch.load(\"checkpoints/llama_instruct_value_assign_finetuned_hooked_new.weights\", weights_only=True))\n",
    "#config = transformer_lens.loading.get_pretrained_model_config(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "#model = transformer_lens.HookedTransformer(config)\n",
    "#model.load_and_process_state_dict(torch.load(\"checkpoints/Qwen2.5-1.5B-Instruct_value_assign_finetuned_hooked.weights\", weights_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0-15): 16 x TransformerBlock(\n",
      "      (ln1): RMSNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): RMSNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): GroupedQueryAttention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "        (hook_rot_k): HookPoint()\n",
      "        (hook_rot_q): HookPoint()\n",
      "      )\n",
      "      (mlp): GatedMLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_pre_linear): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_mid): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): RMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "charset_from = \"\"\n",
    "for id in range(0, 50000):\n",
    "    token = model.tokenizer.decode(id, skip_special_tokens=True)\n",
    "    if len(token) > 1 or token == \"�\":\n",
    "        continue\n",
    "    decode_id = model.tokenizer.encode(token, add_special_tokens=False)\n",
    "    if len(decode_id) == 1 and decode_id[0] == id:\n",
    "        charset_from += token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charset_from[:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "sys.path.insert(0, \"../generators\")\n",
    "sys.path.insert(0, \"generators\")\n",
    "\n",
    "\n",
    "from generators.value_assignment import ValueAssignmentGenerator\n",
    "import transformers\n",
    "import itertools\n",
    "\n",
    "\n",
    "data_generator = ValueAssignmentGenerator(tokenizer=model.tokenizer,\n",
    "                                       seed=0,\n",
    "                                       use_few_shot=True, \n",
    "                                       use_instruction=True, \n",
    "                                       apply_chat_template=True,\n",
    "                                       length=(800, 800),\n",
    "                                       encoding_length=(100, 100),\n",
    "                                       )\n",
    "samples = itertools.islice(data_generator.generate_samples(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(samples)\n",
    "texts = [sample[0] for sample in samples]\n",
    "input_ids = [sample[1] for sample in samples]\n",
    "target_ids = [sample[2] for sample in samples]\n",
    "attn_labels = [sample[3] for sample in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 16 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\\nYou are a great problem solver excellent in tasks requiring manipulation of symbols. You will be tasked with encoding a sequence of characters given a set of encodings. \\nAn encoding is a tuple of characters, e.g. A1, where A is the character to be encoded, and 1 is its encoding. For example, given some string \"A\", its encoding is a sequence \"1\". \\nGiven a set of these encodings and an input string, encode this string.\\nDo not generate any words. You can only generate characters from the set of encodings.\\nHere are some examples:\\nH3K8U1D5G9F1 HKKHDFG=3883519\\nABCDEFGHIJKL ACEGIK=BDFHJLUse them to solve the following task:\\n\\x044W%&+Czcf`xUSH{_dODlc<\\tb?2<Fw|l-VqkBDfZo`3uIsyz*\\x07LN^+iJro\\x01/Y_Jl;qhK\\x029Z@d#\\x07~(C\"/,{P=\\tP:\\x07\\x006VKTWp\\nj(!-s\\x06A36z0=\\x05&5\\n]j%rme{n/x[*x$D0>r\\x089\\x03nRjw0\\\\S\\',4Pt/7%$nz#+9n8g|e0E^u;k~K\\x08M(#wQ#~f\\x06WSp\\n-?B.)X=N*8svG1M9X}8 sCl9fZ7K?-Rv`RM\\x02L{\\x00qp,]sJZKL]W$hcr*8ZrZNDx4(P\\x08\\x04fhyD\\x07~]P,\\'P\\x03c\\x00L|6j]z\\x06!JTE~uB}D_t9,RJ34\\x04k6O<>~i1l#\"_iVeQ26\\x08\\x02~iWcw5NX\\nc$\\'\\x084\\n>\\x070om\\x04|hg4hxc-9r\"\\x05\\x01pycq6H,;PO\\npM1\\x02p\\\\\\tY7S?x\\x071DU_$U\\x04\"A\\x00jmR\\td9^n:ZUQ\\'\\nVR<\"ocN3Ff\\x07\\x05|99F,R~!?;!!Ro/3\\x02vSuCFLgMm4EU_QCIcWMh-F\\x08vs\\ns%REz}$\\no0{kje(m#e\\x08\\x04BsLU#NY,uU>brk`CP!Q4R|9P[(cp|C[xl\\x05\\x02(n!\\x06[TDhbwxgb?e?~wuAgBg\\x00zVgHcXj\\nN\\x03%3Ou\\x06n\\\\\\tY<\\t].XPOn7C\\x00wC?vtHHZ&\\x02\\x041uHSooRXb\\x04d&/#B<\\x0282vdp\\x01X8DB}lm|SNEJ*z\\x01Pc2&HH\\t5$F\\tD_\\x00dN`,Pt;o\\x01p8ei5we\\x00\"+:*VXFTDjo\\x04\\\\[\\x04\\x01+x\\x05|\\'pR+y+r9b\\x06cL6m\"lLrz\\n{CeYh0*?-PrA%x0PD,ng\\x01\\x07x|\\\\({2\\x03ZpPp\\\\J\\x08UY$`x&p\\x06W6UQw$\\x03\\x04cx}~\\\\tIzrlTh-\\tS:\\x00\\'6b?\\x00g(E(IfNmc(y_B`Dq[[\\'Ar~ZVDA88.Xm!\\'3^c-\\x04\\x08Fgw1\".Vhl\\x06e;V;&cdo6BoR7J#L\\x024cXHTtcE1T<_O+P?\\x00H\\tzkCB!b~XJ:5r\\x05o\\x01\\x00/0^y<gO7*r7\\x08$A#b><7ZR\\'\\x03IK*ARyxrqE5;m&b\"1AH+gn]\\\\37uzZ{x\\nlK6ZVWI\\x08\\x03!8jJML/\\'kg(`Ctv\\x02\\x00t\\x08^RYZ07|&x&cI\\x08UU1\\tS3:Zq;9$(,^07wTc.,\"\\x042QA7FhMk*&E-gw-H.{xV|/LE7&XC_\\x01|yZ=<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\\x06zcXZ@%\\x08BVjGxj(9Nn6k\\n{j\\x06l@\\x08Nj%nKfo\\x07s@o@*0$PC=94ZKz0~fj={,=nf6Nlz(j#W-lW^f;D80d/X{jluP4~zD\\trfJMcw/dJK0#<z99fJ%f0\\n*=-fn,9P-r~=`e4lK|PK$fVXo/&/\\nzfkz{{q=D-\\n(M9\\nSP_%pB$~M0SdnS4/36(ejP#X+8\\x07@S#,-Kj\\t/`f*uwZ~&lXXw{jf-Bq--j`xu9Gp;zwN|(eP^Sd#zsf%(KVw9G\\x06-\\x06rj^#8n-`=n~(0Cew094D\\x06NSw*_{;Sr?o~xz=-#PjlX=*Cf\\nlz*$c&9C8-W*W0K?0$|?B0Bf0;3|D|6#K|{f=(-*nruD;W8SP_\\tPj)==D8%z60zBG/{{@+94M;{p``j=?4#+xwD\\t9s<G#\\n/=s0D8celp*^l\\x07#/=f<+{{P\\nnwP0d6#*x{=/q`/\\ns0J\\n006/9\\x07\\x07K=wW0(`4S*4/9$&l,\\nj9z9oX?WfNze/cNo#-nz0_K=\\x07BV=o3r$==0{8|/~$lSCn<n@\\n=\\nSl9S_nx$+\\nW%zS#0nn4f$8fS/s#ocWKVPp\\x076,z?B6|C^CsZ*efCzdDx0k**,3of@K03ss)=e-,u+fV49w|0M/)KKcW0qKq+f#`zD`j%lwN9Pf={W/f^MW\\tdD9=B6{P#~zD-?f=l\\x07\\no&`/6x=+z\\t|D%\\x07o%9n3w?r\\t%@j,ns\\x08\\x073jz$ok^\\nqe+?/M3{9|8jSu%;#@n$-c\\x08z@K%s9n-s(l(Nx,~|Cxz/G96/9+j_@=%l+$+fs9SSMPpu\\x07@kqXnC{+=%0Wf){/4<#3%wK(~\\x07+^V|0V{)n$KlxN^%+=zd/lz@<|eot_id|>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def visualize_attention_heads(attention_matrix, tokens=None, title=\"Attention Heatmap\", save_path=None, heads=None, length=None, attn_labels=None):\n",
    "    \"\"\"\n",
    "    Visualize a multi-head attention matrix as heatmaps for each head.\n",
    "\n",
    "    Parameters:\n",
    "        attention_matrix (torch.Tensor): Tensor of shape (1, num_heads, seq_len, seq_len).\n",
    "        tokens (list, optional): List of tokens corresponding to the rows/columns of the matrix.\n",
    "        title (str): Title of the heatmap.\n",
    "        save_path (str, optional): Path to save the heatmap images. If None, the heatmaps are not saved.\n",
    "        heads (list, optional): List of head indices to visualize.\n",
    "        length (int, optional): Number of tokens to display from the lower-right corner.\n",
    "\"\"\"\n",
    "    scores = attention_matrix.squeeze(0).cpu().detach().numpy()  # Shape: (num_heads, seq_len, seq_len)\n",
    "    num_heads = scores.shape[0]\n",
    "    seq_len = scores.shape[1]\n",
    "\n",
    "    for head in range(num_heads):\n",
    "        if heads is not None and head not in heads:\n",
    "            continue\n",
    "        head_scores = scores[head]  # Shape: (seq_len, seq_len)\n",
    "\n",
    "        # Apply filtering for the lower-right \"length x length\" square\n",
    "        if length is not None and length < seq_len:\n",
    "            head_scores = head_scores[-length:, -length:]\n",
    "            if tokens:\n",
    "                tokens = tokens[-length:]\n",
    "            if attn_labels is not None:\n",
    "                attn_labels = attn_labels[-length:, -length:]\n",
    "\n",
    "        # Normalize scores for better visualization\n",
    "        min_score = head_scores.min()\n",
    "        max_score = head_scores.max()\n",
    "        normalized = (head_scores - min_score) / (max_score - min_score)  # Normalize to [0, 1]\n",
    "\n",
    "        # Create the heatmap\n",
    "        fig = go.Figure(\n",
    "            data=go.Heatmap(\n",
    "                z=normalized,\n",
    "                x=tokens if tokens else list(range(len(normalized))),\n",
    "                y=tokens if tokens else list(range(len(normalized))),\n",
    "                colorscale=\"Viridis\",\n",
    "                colorbar=dict(title=\"Attention Score\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add title and layout\n",
    "        fig.update_layout(\n",
    "            title=f\"{title} - Head {head + 1}\",\n",
    "            xaxis=dict(title=\"Tokens\", tickangle=45),\n",
    "            yaxis=dict(title=\"Tokens\"),\n",
    "        )\n",
    "\n",
    "        # Highlight attention label regions if provided\n",
    "        if attn_labels is not None:\n",
    "            for i in range(len(attn_labels)):\n",
    "                for j in range(len(attn_labels)):\n",
    "                    if attn_labels[i, j] == 1:\n",
    "                        fig.add_shape(\n",
    "                            type=\"rect\",\n",
    "                            x0=j - 0.5,\n",
    "                            x1=j + 0.5,\n",
    "                            y0=i - 0.5,\n",
    "                            y1=i + 0.5,\n",
    "                            line=dict(color=\"red\", width=2),\n",
    "                        )\n",
    "\n",
    "        # Save the plot if a save path is provided\n",
    "        if save_path:\n",
    "            fig.write_image(f\"{save_path}_head_{head + 1}.png\")\n",
    "\n",
    "        # Show the plot\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "0.0003\n",
      "Loss: 3.4059817790985107, Accuracy: 38.08%\n",
      "Moving model to device:  cuda\n",
      "0.0003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m heads_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(heads_to_reinforce_x)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(threshold)\n\u001b[0;32m---> 91\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mfwd_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_attn_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfix_attn_pattern_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_labels_1_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_labels_1_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheads_to_reinforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheads_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_last_n_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkeep_last_n_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m loss, acc \u001b[38;5;241m=\u001b[39m compute_scores(logits, target_ids[idx])\n\u001b[1;32m     96\u001b[0m ablated\u001b[38;5;241m.\u001b[39mappend((heads_x, loss, acc))\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/transformer_lens/hook_points.py:456\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_hooks\u001b[0;34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts) \u001b[38;5;28;01mas\u001b[39;00m hooked_model:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhooked_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:612\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    609\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    610\u001b[0m         )\n\u001b[0;32m--> 612\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/transformer_lens/components/transformer_block.py:160\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    153\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    154\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    156\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_post(attn_out)\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/transformer_lens/components/abstract_attention.py:261\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    259\u001b[0m pattern \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attn_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    260\u001b[0m pattern \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(torch\u001b[38;5;241m.\u001b[39misnan(pattern), torch\u001b[38;5;241m.\u001b[39mzeros_like(pattern), pattern)\n\u001b[0;32m--> 261\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_pattern\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, head_index, query_pos, key_pos]\u001b[39;00m\n\u001b[1;32m    262\u001b[0m pattern \u001b[38;5;241m=\u001b[39m pattern\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    263\u001b[0m pattern \u001b[38;5;241m=\u001b[39m pattern\u001b[38;5;241m.\u001b[39mto(v\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1806\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1804\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1806\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1809\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/nlp/projekty/aver/xspiege1/algorithmic_reasoning/.env/lib/python3.10/site-packages/transformer_lens/hook_points.py:109\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbwd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m ):  \u001b[38;5;66;03m# For a backwards hook, module_output is a tuple of (grad,) - I don't know why.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     module_output \u001b[38;5;241m=\u001b[39m module_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 46\u001b[0m, in \u001b[0;36mfix_attn_pattern_hook\u001b[0;34m(value, hook, attn_labels_1_idx, heads_to_reinforce, attn_labels, keep_last_n_tokens, threshold)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (i, j) \u001b[38;5;129;01min\u001b[39;00m attn_labels_1_idx \u001b[38;5;129;01mand\u001b[39;00m value[\u001b[38;5;241m0\u001b[39m, h, i, j] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold:\n\u001b[1;32m     47\u001b[0m             value[\u001b[38;5;241m0\u001b[39m, h, i, j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;66;03m#acc += value[0, h, i, j]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_heads = {}\n",
    "\n",
    "def compute_scores(logits, target):\n",
    "    ignore_index = -100  # Optional: index to ignore in loss/accuracy computation\n",
    "    # Compute loss\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "    logits.view(-1, logits.size(-1)),  # Flatten logits to (batch_size * seq_len, vocab_size)\n",
    "    target.view(-1),               # Flatten target_ids to (batch_size * seq_len)\n",
    "    ignore_index=ignore_index          # Ignore padding tokens if applicable\n",
    ")\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = torch.argmax(logits, dim=-1)  # Get predicted token indices\n",
    "    correct = (predictions == target) & (target != ignore_index)  # Ignore padding tokens\n",
    "    accuracy = correct.sum().item() / (target != ignore_index).sum().item()\n",
    "\n",
    "    print(f\"Loss: {loss.item()}, Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    errors = (predictions != target) & (target != ignore_index)  # Identify error positions\n",
    "    errors_np = errors.cpu().numpy()  # Convert to NumPy for visualization\n",
    "    #print(\"Errors:\", errors_np)\n",
    "\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "def fix_attn_pattern_hook(\n",
    "    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint,\n",
    "    attn_labels_1_idx,\n",
    "    heads_to_reinforce,\n",
    "    attn_labels,\n",
    "    keep_last_n_tokens,\n",
    "    threshold=3e-3\n",
    ") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
    "    batch, heads, seq_len, _ = value.shape \n",
    "    layer = int(hook.name.split(\".\")[1])\n",
    "    if layer not in heads_to_reinforce.keys():\n",
    "        return\n",
    "    \n",
    "    for h in range(heads):\n",
    "        if h not in heads_to_reinforce[layer]:\n",
    "            continue\n",
    "        acc = 0\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            for j in range(seq_len):\n",
    "                if (i, j) in attn_labels_1_idx and value[0, h, i, j] >= threshold:\n",
    "                    value[0, h, i, j] += 1\n",
    "                    #acc += value[0, h, i, j]\n",
    "                    pass\n",
    "        #best_heads[(layer, h)] = best_heads.get((layer, h), 0) + acc\n",
    "        #visualize_attention_heads(value, None, title=f\"Example Attention Heatmap Layer {layer}\", heads=[h], length=keep_last_n_tokens, attn_labels=attn_labels)\n",
    "\n",
    "    return value\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "is_attn_scores = lambda name: name.endswith(\"hook_pattern\")\n",
    "ablated = []\n",
    "for idx in range(0, 3):\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    indices =  np.argwhere(attn_labels[idx].cpu().numpy() == 1)\n",
    "    attn_labels_1_idx = set(map(tuple, indices))\n",
    "    \n",
    "\n",
    "    model.to(device)\n",
    "    input_ids[idx] = input_ids[idx].to(device)\n",
    "    target_ids[idx] = target_ids[idx].to(device)\n",
    "    attn_labels[idx] = attn_labels[idx].to(device)\n",
    "                \n",
    "\n",
    "    heads_to_reinforce_x=[\n",
    "        (7, [5]),\n",
    "        (8, [20, 31]),\n",
    "        (9, [8, 10]),\n",
    "        (10, [23, 20, 21, 1, 22]),\n",
    "        (11, [14, 20, 4]),\n",
    "        (12, [13, 2, 29]),\n",
    "        (13, [20, 21, 22, 2, 23, 31, 1, 11]),\n",
    "        (14, [22, 11, 20, 15, 21, 14, 31, 3, 28, 29]),\n",
    "        (15, [31, 7, 22, 30, 20, 29]),\n",
    "    ]\n",
    "    threshold = 0.02\n",
    "    for threshold in np.linspace(0.02, 0.0017145714285714286, 18):\n",
    "        cols_to_keep = ~np.all(attn_labels[idx].cpu().numpy() == -100, axis=0)\n",
    "        keep_last_n_tokens = 400\n",
    "        heads_x = dict(heads_to_reinforce_x)\n",
    "        print(threshold)\n",
    "        logits = model.run_with_hooks(input_ids[idx], return_type=\"logits\",\n",
    "                                        fwd_hooks=[\n",
    "                                            (is_attn_scores, partial(fix_attn_pattern_hook, attn_labels_1_idx=attn_labels_1_idx, heads_to_reinforce=heads_x, attn_labels=attn_labels[idx], keep_last_n_tokens = keep_last_n_tokens, threshold=threshold))\n",
    "                                        ])\n",
    "        loss, acc = compute_scores(logits, target_ids[idx])\n",
    "        ablated.append((heads_x, loss, acc))\n",
    "        del logits\n",
    "        torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5577114427860697\n",
      "5.577114427860696\n"
     ]
    }
   ],
   "source": [
    "sums = 0\n",
    "for item in ablated:\n",
    "    sums += item[2]\n",
    "print(sums/len(ablated))\n",
    "print(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 23 144.37127685546875\n",
      "10 20 143.6128387451172\n",
      "10 21 133.88848876953125\n",
      "10 1 126.30549621582031\n",
      "7 5 114.27082824707031\n",
      "13 20 108.69319152832031\n",
      "9 8 105.45207977294922\n",
      "12 13 104.23811340332031\n",
      "13 21 99.49042510986328\n",
      "14 22 76.86735534667969\n",
      "14 11 74.93190002441406\n",
      "13 22 68.8270492553711\n",
      "13 2 64.8114242553711\n",
      "9 10 63.97957992553711\n",
      "11 14 62.73735809326172\n",
      "14 20 59.85484313964844\n",
      "13 23 59.02810287475586\n",
      "11 20 49.786563873291016\n",
      "11 4 49.47258758544922\n",
      "14 15 48.54301452636719\n",
      "15 31 47.76313781738281\n",
      "12 2 46.754432678222656\n",
      "14 21 46.222862243652344\n",
      "14 14 42.85946273803711\n",
      "12 29 42.85609436035156\n",
      "15 7 41.5084342956543\n",
      "13 31 40.21308517456055\n",
      "8 20 39.4764518737793\n",
      "14 31 37.61896896362305\n",
      "8 31 36.4268913269043\n",
      "10 22 36.38793182373047\n",
      "13 1 35.12792205810547\n",
      "14 3 34.78273010253906\n",
      "15 22 34.29121017456055\n",
      "15 21 34.25675582885742\n",
      "14 28 34.11720275878906\n",
      "13 11 34.04398727416992\n",
      "14 29 33.27365493774414\n",
      "15 30 33.105934143066406\n",
      "15 20 32.20920181274414\n",
      "15 29 31.957857131958008\n",
      "9 16 31.616771697998047\n",
      "14 30 30.305744171142578\n",
      "13 0 29.88428497314453\n",
      "8 13 29.497787475585938\n",
      "12 15 28.952320098876953\n",
      "7 30 27.943805694580078\n",
      "15 17 26.41773223876953\n",
      "11 23 25.436716079711914\n",
      "15 4 24.247753143310547\n",
      "9 24 22.197328567504883\n",
      "12 31 22.029836654663086\n",
      "8 21 21.665523529052734\n",
      "8 12 21.467830657958984\n",
      "13 10 20.535219192504883\n",
      "12 14 20.441593170166016\n",
      "11 22 20.383848190307617\n",
      "7 28 20.076942443847656\n",
      "15 12 19.97838020324707\n",
      "14 12 19.464847564697266\n",
      "11 18 19.006874084472656\n",
      "8 30 18.059906005859375\n",
      "11 6 18.045795440673828\n",
      "14 2 17.183246612548828\n",
      "14 9 17.109254837036133\n",
      "15 16 16.514663696289062\n",
      "15 19 15.410840034484863\n",
      "14 27 14.805234909057617\n",
      "12 12 14.633191108703613\n",
      "8 23 13.163154602050781\n",
      "11 12 12.441976547241211\n",
      "10 9 12.263659477233887\n",
      "7 29 12.110315322875977\n",
      "14 8 11.637264251708984\n",
      "11 5 11.386869430541992\n",
      "10 13 11.142545700073242\n",
      "12 28 11.017847061157227\n",
      "10 14 10.86275577545166\n",
      "11 13 10.72365951538086\n",
      "14 13 10.66054630279541\n",
      "15 18 10.635760307312012\n",
      "12 18 10.147735595703125\n",
      "8 19 10.059579849243164\n",
      "7 6 9.709300994873047\n",
      "15 28 9.537160873413086\n",
      "13 3 9.137491226196289\n",
      "8 18 8.884974479675293\n",
      "6 10 8.86997127532959\n",
      "10 30 8.67528247833252\n",
      "6 11 8.618881225585938\n",
      "4 16 8.572294235229492\n",
      "8 17 8.20644474029541\n",
      "11 21 8.14426040649414\n",
      "14 16 7.900679588317871\n",
      "11 7 7.697556972503662\n",
      "10 24 7.642175674438477\n",
      "5 5 7.038971424102783\n",
      "2 26 6.888570785522461\n",
      "6 31 6.679655075073242\n",
      "9 13 6.433203220367432\n",
      "6 8 6.369867324829102\n",
      "11 10 6.063967227935791\n",
      "9 21 5.857753753662109\n",
      "2 15 5.5557861328125\n",
      "2 14 5.544702529907227\n",
      "13 16 5.480151653289795\n",
      "15 5 5.2565016746521\n",
      "14 0 5.111028671264648\n",
      "15 13 5.087508678436279\n",
      "9 4 4.8010663986206055\n",
      "7 22 4.607081413269043\n",
      "11 27 4.416594505310059\n",
      "1 23 4.4165802001953125\n",
      "10 25 4.266080379486084\n",
      "15 15 4.248216152191162\n",
      "8 28 4.217863082885742\n",
      "13 28 4.194703102111816\n",
      "13 13 3.943197011947632\n",
      "15 11 3.8373589515686035\n",
      "15 2 3.781449556350708\n",
      "10 15 3.753120183944702\n",
      "12 27 3.711655855178833\n",
      "7 26 3.6901566982269287\n",
      "7 12 3.660029888153076\n",
      "10 10 3.6375577449798584\n",
      "2 20 3.548675060272217\n",
      "14 25 3.544722557067871\n",
      "13 17 3.479572057723999\n",
      "7 24 3.4026939868927\n",
      "6 30 3.3539493083953857\n",
      "4 19 3.305051565170288\n",
      "9 15 3.2885055541992188\n",
      "5 10 3.285961627960205\n",
      "8 24 3.2473371028900146\n",
      "15 27 3.2444286346435547\n",
      "5 7 3.1225266456604004\n",
      "10 0 2.979205369949341\n",
      "14 18 2.777296781539917\n",
      "8 27 2.775989055633545\n",
      "12 4 2.753000020980835\n",
      "0 9 2.715214729309082\n",
      "15 1 2.6761634349823\n",
      "10 26 2.670152187347412\n",
      "6 9 2.5078072547912598\n",
      "15 8 2.470888137817383\n",
      "13 14 2.4536075592041016\n",
      "14 17 2.441967487335205\n",
      "3 27 2.4178688526153564\n",
      "0 20 2.404594659805298\n",
      "6 28 2.394529342651367\n",
      "12 5 2.373568534851074\n",
      "0 23 2.3077259063720703\n",
      "15 0 2.3017630577087402\n",
      "7 7 2.2839858531951904\n",
      "10 11 2.2600338459014893\n",
      "7 13 2.243339776992798\n",
      "11 3 2.208193063735962\n",
      "4 29 2.201308012008667\n",
      "2 19 2.043792247772217\n",
      "7 27 2.043311834335327\n",
      "1 12 2.0311715602874756\n",
      "5 17 2.0118582248687744\n",
      "9 27 2.0040464401245117\n",
      "14 19 1.9986529350280762\n",
      "10 6 1.9659802913665771\n",
      "15 3 1.9617321491241455\n",
      "6 19 1.9432528018951416\n",
      "13 30 1.9355175495147705\n",
      "13 5 1.929140567779541\n",
      "12 24 1.905440092086792\n",
      "14 26 1.8506944179534912\n",
      "0 21 1.8421542644500732\n",
      "6 29 1.8420600891113281\n",
      "8 22 1.8331800699234009\n",
      "1 21 1.8173854351043701\n",
      "8 15 1.8107188940048218\n",
      "13 19 1.7334496974945068\n",
      "4 0 1.7255263328552246\n",
      "6 4 1.6423066854476929\n",
      "0 8 1.6339150667190552\n",
      "5 21 1.5563390254974365\n",
      "12 7 1.5431461334228516\n",
      "8 3 1.5419299602508545\n",
      "7 25 1.5340272188186646\n",
      "9 0 1.518339991569519\n",
      "11 24 1.5142738819122314\n",
      "10 27 1.5044116973876953\n",
      "0 27 1.5039136409759521\n",
      "6 16 1.5030076503753662\n",
      "0 15 1.4966076612472534\n",
      "7 15 1.4660353660583496\n",
      "0 26 1.4513858556747437\n",
      "2 6 1.4443881511688232\n",
      "11 25 1.44415283203125\n",
      "0 17 1.4192585945129395\n",
      "2 5 1.3761603832244873\n",
      "8 25 1.3674421310424805\n",
      "7 23 1.366464614868164\n",
      "9 31 1.3491542339324951\n",
      "8 5 1.33417809009552\n",
      "12 6 1.3279452323913574\n",
      "7 14 1.3278928995132446\n",
      "0 12 1.3261773586273193\n",
      "0 13 1.3193336725234985\n",
      "7 3 1.2875663042068481\n",
      "8 1 1.2865344285964966\n",
      "0 25 1.2698962688446045\n",
      "4 3 1.2578892707824707\n",
      "12 26 1.2476811408996582\n",
      "13 29 1.242018699645996\n",
      "8 29 1.2368159294128418\n",
      "5 23 1.2346302270889282\n",
      "0 14 1.2262914180755615\n",
      "0 24 1.2077248096466064\n",
      "12 25 1.2014423608779907\n",
      "15 26 1.1708258390426636\n",
      "14 6 1.1531435251235962\n",
      "0 22 1.1489139795303345\n",
      "8 14 1.112160086631775\n",
      "1 22 1.1008392572402954\n",
      "10 12 1.095106840133667\n",
      "8 0 1.085118055343628\n",
      "7 20 1.0738450288772583\n",
      "4 15 1.056947946548462\n",
      "7 21 1.0556640625\n",
      "3 9 1.0443562269210815\n",
      "5 8 1.0359601974487305\n",
      "7 9 1.0346866846084595\n",
      "6 12 1.0309351682662964\n",
      "9 6 1.0255671739578247\n",
      "4 6 1.0100111961364746\n",
      "9 2 0.9905846118927002\n",
      "0 4 0.9889729619026184\n",
      "0 5 0.9627976417541504\n",
      "6 27 0.942190945148468\n",
      "13 4 0.9408531188964844\n",
      "0 7 0.9328996539115906\n",
      "0 28 0.8935264945030212\n",
      "6 18 0.8810556530952454\n",
      "0 6 0.8732635974884033\n",
      "0 19 0.8673129081726074\n",
      "7 10 0.8655821681022644\n",
      "4 5 0.8563015460968018\n",
      "5 22 0.8546255826950073\n",
      "1 28 0.848546028137207\n",
      "5 27 0.8421383500099182\n",
      "5 0 0.8417961597442627\n",
      "6 25 0.8365304470062256\n",
      "6 14 0.8162769675254822\n",
      "0 18 0.8155779242515564\n",
      "6 26 0.8152532577514648\n",
      "6 5 0.8138352632522583\n",
      "5 4 0.8120517730712891\n",
      "8 7 0.7941910028457642\n",
      "9 3 0.7900509834289551\n",
      "4 10 0.7881789803504944\n",
      "7 2 0.7877376079559326\n",
      "6 20 0.7789773941040039\n",
      "0 11 0.7746613621711731\n",
      "10 31 0.7668440341949463\n",
      "7 11 0.7571231722831726\n",
      "8 2 0.7486327886581421\n",
      "5 9 0.7414321899414062\n",
      "5 6 0.7412247657775879\n",
      "7 18 0.7395120859146118\n",
      "5 18 0.7354384064674377\n",
      "14 4 0.7288255095481873\n",
      "3 12 0.7275538444519043\n",
      "8 4 0.7102026343345642\n",
      "7 19 0.703994631767273\n",
      "4 8 0.6897739171981812\n",
      "7 1 0.6786887645721436\n",
      "15 6 0.6683828830718994\n",
      "3 22 0.6514878273010254\n",
      "6 23 0.6514634490013123\n",
      "0 16 0.6487932205200195\n",
      "0 1 0.6472780108451843\n",
      "2 23 0.633461594581604\n",
      "14 10 0.6258935332298279\n",
      "9 14 0.6252923607826233\n",
      "9 17 0.6168131232261658\n",
      "13 18 0.6132644414901733\n",
      "6 24 0.5873026847839355\n",
      "3 1 0.5769934058189392\n",
      "2 13 0.5727176070213318\n",
      "0 10 0.5666239261627197\n",
      "4 20 0.5651224255561829\n",
      "11 17 0.5649522542953491\n",
      "3 13 0.5627630352973938\n",
      "2 3 0.5370368361473083\n",
      "15 9 0.5292711853981018\n",
      "11 2 0.5204873085021973\n",
      "3 29 0.5180618166923523\n",
      "12 0 0.5134299993515015\n",
      "8 6 0.5131872892379761\n",
      "9 23 0.5095416307449341\n",
      "5 1 0.5004975199699402\n",
      "5 19 0.49950024485588074\n",
      "6 6 0.4926607310771942\n",
      "12 10 0.4904133081436157\n",
      "9 22 0.48489245772361755\n",
      "2 4 0.4827258884906769\n",
      "0 30 0.4819176197052002\n",
      "11 1 0.4703874886035919\n",
      "9 9 0.4529462158679962\n",
      "5 16 0.44834399223327637\n",
      "3 30 0.4435470402240753\n",
      "1 29 0.4364722669124603\n",
      "6 7 0.4261476993560791\n",
      "10 18 0.41910436749458313\n",
      "3 25 0.41033464670181274\n",
      "2 24 0.4093873202800751\n",
      "14 5 0.39206573367118835\n",
      "2 17 0.388639897108078\n",
      "5 24 0.3770000636577606\n",
      "12 30 0.3766670227050781\n",
      "13 6 0.37535548210144043\n",
      "11 15 0.36987707018852234\n",
      "4 1 0.36604270339012146\n",
      "2 10 0.36360159516334534\n",
      "14 24 0.3613998293876648\n",
      "4 2 0.356520414352417\n",
      "9 1 0.35418444871902466\n",
      "4 7 0.3511534333229065\n",
      "6 17 0.3456227481365204\n",
      "7 8 0.33600789308547974\n",
      "5 11 0.33046284317970276\n",
      "14 1 0.33032214641571045\n",
      "5 26 0.32806384563446045\n",
      "14 23 0.32614415884017944\n",
      "15 24 0.3167400062084198\n",
      "11 26 0.31378522515296936\n",
      "1 9 0.3116488754749298\n",
      "13 9 0.31159311532974243\n",
      "2 27 0.307250440120697\n",
      "15 10 0.30420002341270447\n",
      "4 22 0.3014436364173889\n",
      "11 0 0.2916589677333832\n",
      "1 31 0.2865760028362274\n",
      "3 23 0.2858368754386902\n",
      "7 4 0.28275179862976074\n",
      "1 8 0.2807009220123291\n",
      "4 25 0.2783374488353729\n",
      "13 7 0.2764590084552765\n",
      "13 12 0.27024707198143005\n",
      "5 28 0.2654743790626526\n",
      "13 15 0.26508602499961853\n",
      "4 23 0.2585148811340332\n",
      "15 25 0.2563408315181732\n",
      "3 0 0.2537098824977875\n",
      "10 2 0.24791567027568817\n",
      "14 7 0.23691114783287048\n",
      "2 12 0.22947557270526886\n",
      "9 18 0.2256222516298294\n",
      "7 0 0.2068832814693451\n",
      "9 25 0.20602795481681824\n",
      "3 20 0.2028408944606781\n",
      "4 11 0.1995280534029007\n",
      "5 15 0.18938525021076202\n",
      "2 16 0.18779608607292175\n",
      "0 0 0.18477492034435272\n",
      "4 9 0.18252897262573242\n",
      "6 13 0.17824387550354004\n",
      "2 1 0.17735044658184052\n",
      "1 14 0.1743496060371399\n",
      "4 28 0.17364725470542908\n",
      "4 21 0.1706259399652481\n",
      "3 28 0.16693057119846344\n",
      "3 24 0.16580598056316376\n",
      "8 10 0.16514956951141357\n",
      "3 21 0.16131073236465454\n",
      "5 31 0.15978668630123138\n",
      "8 16 0.15466220676898956\n",
      "4 4 0.1540539413690567\n",
      "5 30 0.14711466431617737\n",
      "4 30 0.14372900128364563\n",
      "7 17 0.14127352833747864\n",
      "0 29 0.1395668387413025\n",
      "0 31 0.1391850709915161\n",
      "10 8 0.13892415165901184\n",
      "3 15 0.13692203164100647\n",
      "10 19 0.13520510494709015\n",
      "10 17 0.13190165162086487\n",
      "9 20 0.1288134753704071\n",
      "2 29 0.12144327163696289\n",
      "4 26 0.11512810736894608\n",
      "15 23 0.11409725993871689\n",
      "6 21 0.11361866444349289\n",
      "9 26 0.11036320775747299\n",
      "2 7 0.10414720326662064\n",
      "12 16 0.1032036542892456\n",
      "5 12 0.1008489727973938\n",
      "2 21 0.10036762803792953\n",
      "9 7 0.09946610033512115\n",
      "6 15 0.09550054371356964\n",
      "1 2 0.09483151137828827\n",
      "0 3 0.09052632749080658\n",
      "3 14 0.08879780769348145\n",
      "7 31 0.0879775658249855\n",
      "3 31 0.08665261417627335\n",
      "2 2 0.08625700324773788\n",
      "7 16 0.08422799408435822\n",
      "12 20 0.07865419238805771\n",
      "1 15 0.07833588868379593\n",
      "4 27 0.07610025256872177\n",
      "9 5 0.07605435699224472\n",
      "11 19 0.06761500984430313\n",
      "10 3 0.06475584208965302\n",
      "1 10 0.06229134649038315\n",
      "13 8 0.05898870900273323\n",
      "6 22 0.053630709648132324\n",
      "2 0 0.05042252689599991\n",
      "10 28 0.048624537885189056\n",
      "2 22 0.04744187369942665\n",
      "12 19 0.04490326717495918\n",
      "4 18 0.043626897037029266\n",
      "0 2 0.04268820211291313\n",
      "3 26 0.04243740439414978\n",
      "5 29 0.037924595177173615\n",
      "12 17 0.035511814057826996\n",
      "5 3 0.03170479089021683\n",
      "10 29 0.029195038601756096\n",
      "12 9 0.028594722971320152\n",
      "4 31 0.027659164741635323\n",
      "11 8 0.025991784408688545\n",
      "9 29 0.025766532868146896\n",
      "3 19 0.024479031562805176\n",
      "1 13 0.02437739074230194\n",
      "2 9 0.02358204498887062\n",
      "5 2 0.023209067061543465\n",
      "4 12 0.022996390238404274\n",
      "2 28 0.02291133441030979\n",
      "1 30 0.022602351382374763\n",
      "3 7 0.022586574777960777\n",
      "5 14 0.022372785955667496\n",
      "10 4 0.022193685173988342\n",
      "2 8 0.021290894597768784\n",
      "13 26 0.02045336924493313\n",
      "2 18 0.01973363198339939\n",
      "5 20 0.019658826291561127\n",
      "12 21 0.019482441246509552\n",
      "9 19 0.018877673894166946\n",
      "12 22 0.018522795289754868\n",
      "5 13 0.017706552520394325\n",
      "13 27 0.017598453909158707\n",
      "1 11 0.01751003973186016\n",
      "15 14 0.017506718635559082\n",
      "1 19 0.017477450892329216\n",
      "5 25 0.016740212216973305\n",
      "3 2 0.01527768187224865\n",
      "3 10 0.01494380459189415\n",
      "2 11 0.01341284066438675\n",
      "2 25 0.013268046081066132\n",
      "9 12 0.013003328815102577\n",
      "10 16 0.012690749950706959\n",
      "8 26 0.012541725300252438\n",
      "12 23 0.012347270734608173\n",
      "4 14 0.011346379294991493\n",
      "3 11 0.011296110227704048\n",
      "13 25 0.009847106412053108\n",
      "12 1 0.009824126958847046\n",
      "11 11 0.00978117249906063\n",
      "3 3 0.00969488825649023\n",
      "11 9 0.00930666085332632\n",
      "3 8 0.008885095827281475\n",
      "10 7 0.008611406199634075\n",
      "8 8 0.008591231890022755\n",
      "4 13 0.008196476846933365\n",
      "3 6 0.008150405250489712\n",
      "6 1 0.007923755794763565\n",
      "11 16 0.007639226503670216\n",
      "3 18 0.0074508897960186005\n",
      "8 9 0.00734469061717391\n",
      "1 20 0.007029579486697912\n",
      "2 31 0.006983301602303982\n",
      "12 8 0.006849322933703661\n",
      "1 0 0.006777705624699593\n",
      "4 24 0.00657637557014823\n",
      "12 3 0.006492012180387974\n",
      "12 11 0.006212883163243532\n",
      "10 5 0.005933292210102081\n",
      "1 3 0.0059164706617593765\n",
      "13 24 0.005911457352340221\n",
      "3 16 0.005803026724606752\n",
      "1 7 0.0050350939854979515\n",
      "6 2 0.0046257092617452145\n",
      "3 17 0.004115681163966656\n",
      "4 17 0.0038371598348021507\n",
      "1 25 0.003816849086433649\n",
      "1 6 0.0025266697630286217\n",
      "11 29 0.002478874521329999\n",
      "1 17 0.002335143508389592\n",
      "9 30 0.002335132099688053\n",
      "3 4 0.002051316434517503\n",
      "3 5 0.001958353677764535\n",
      "11 28 0.001782542560249567\n",
      "2 30 0.0017640389269217849\n",
      "1 16 0.0017619390273466706\n",
      "1 1 0.0017081421101465821\n",
      "1 24 0.0015007216716185212\n",
      "1 18 0.0014467191649600863\n",
      "9 11 0.0013932997826486826\n",
      "11 30 0.0012934913393110037\n",
      "1 5 0.0012413564836606383\n",
      "1 4 0.001214957213960588\n",
      "6 0 0.0011330099077895284\n",
      "8 11 0.0010130041046068072\n",
      "9 28 0.0006263145478442311\n",
      "11 31 0.0005873109912499785\n",
      "6 3 0.0005490333423949778\n",
      "1 27 0.0004549645527731627\n",
      "1 26 0.00042711186688393354\n"
     ]
    }
   ],
   "source": [
    "for item in sorted(best_heads.items(), key=lambda item: item[1], reverse=True):\n",
    "    print(item[0][0], item[0][1], item[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
