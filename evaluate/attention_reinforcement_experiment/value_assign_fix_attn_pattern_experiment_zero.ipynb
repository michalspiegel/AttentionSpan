{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipykernel transformer_lens plotly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "import torch\n",
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert HF Transformers weights to HookedTransformer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from transformer_lens.pretrained.weight_conversions.llama import convert_llama_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "model.load_state_dict(torch.load(\"checkpoints/llama_instruct_value_assign_finetuned_new.weights\", weights_only=True))\n",
    "hooked_config = transformer_lens.loading.get_pretrained_model_config(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "new_state_dict = convert_llama_weights(model, hooked_config)\n",
    "torch.save(new_state_dict, \"checkpoints/llama_instruct_value_assign_finetuned_hooked_new.weights\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformer_lens.loading.get_pretrained_model_config(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = transformer_lens.HookedTransformer(config)\n",
    "model.load_and_process_state_dict(torch.load(\"checkpoints/llama_instruct_value_assign_finetuned_hooked_new.weights\", weights_only=True))\n",
    "#config = transformer_lens.loading.get_pretrained_model_config(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "#model = transformer_lens.HookedTransformer(config)\n",
    "#model.load_and_process_state_dict(torch.load(\"checkpoints/Qwen2.5-1.5B-Instruct_value_assign_finetuned_hooked.weights\", weights_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charset_from = \"\"\n",
    "for id in range(0, 50000):\n",
    "    token = model.tokenizer.decode(id, skip_special_tokens=True)\n",
    "    if len(token) > 1 or token == \"ï¿½\":\n",
    "        continue\n",
    "    decode_id = model.tokenizer.encode(token, add_special_tokens=False)\n",
    "    if len(decode_id) == 1 and decode_id[0] == id:\n",
    "        charset_from += token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charset_from[:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "sys.path.insert(0, \"../generators\")\n",
    "sys.path.insert(0, \"generators\")\n",
    "\n",
    "\n",
    "from generators.value_assignment import ValueAssignmentGenerator\n",
    "import transformers\n",
    "import itertools\n",
    "\n",
    "\n",
    "data_generator = ValueAssignmentGenerator(tokenizer=model.tokenizer,\n",
    "                                       seed=0,\n",
    "                                       #use_few_shot=True, \n",
    "                                       #use_instruction=True, \n",
    "                                       #apply_chat_template=True,\n",
    "                                       length=(3, 3),\n",
    "                                       encoding_length=(3, 3),\n",
    "                                       )\n",
    "samples = itertools.islice(data_generator.generate_samples(), 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(samples)\n",
    "texts = [sample[0] for sample in samples]\n",
    "input_ids = [sample[1] for sample in samples]\n",
    "target_ids = [sample[2] for sample in samples]\n",
    "attn_labels = [sample[3] for sample in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def visualize_attention_heads(attention_matrix, tokens=None, title=\"Attention Heatmap\", save_path=None, heads=None, length=None, attn_labels=None):\n",
    "    \"\"\"\n",
    "    Visualize a multi-head attention matrix as heatmaps for each head.\n",
    "\n",
    "    Parameters:\n",
    "        attention_matrix (torch.Tensor): Tensor of shape (1, num_heads, seq_len, seq_len).\n",
    "        tokens (list, optional): List of tokens corresponding to the rows/columns of the matrix.\n",
    "        title (str): Title of the heatmap.\n",
    "        save_path (str, optional): Path to save the heatmap images. If None, the heatmaps are not saved.\n",
    "        heads (list, optional): List of head indices to visualize.\n",
    "        length (int, optional): Number of tokens to display from the lower-right corner.\n",
    "\"\"\"\n",
    "    scores = attention_matrix.squeeze(0).cpu().detach().numpy()  # Shape: (num_heads, seq_len, seq_len)\n",
    "    num_heads = scores.shape[0]\n",
    "    seq_len = scores.shape[1]\n",
    "\n",
    "    for head in range(num_heads):\n",
    "        if heads is not None and head not in heads:\n",
    "            continue\n",
    "        head_scores = scores[head]  # Shape: (seq_len, seq_len)\n",
    "\n",
    "        # Apply filtering for the lower-right \"length x length\" square\n",
    "        if length is not None and length < seq_len:\n",
    "            head_scores = head_scores[-length:, -length:]\n",
    "            if tokens:\n",
    "                tokens = tokens[-length:]\n",
    "            if attn_labels is not None:\n",
    "                attn_labels = attn_labels[-length:, -length:]\n",
    "\n",
    "        # Normalize scores for better visualization\n",
    "        min_score = head_scores.min()\n",
    "        max_score = head_scores.max()\n",
    "        normalized = (head_scores - min_score) / (max_score - min_score)  # Normalize to [0, 1]\n",
    "\n",
    "        # Create the heatmap\n",
    "        fig = go.Figure(\n",
    "            data=go.Heatmap(\n",
    "                z=normalized,\n",
    "                x=tokens if tokens else list(range(len(normalized))),\n",
    "                y=tokens if tokens else list(range(len(normalized))),\n",
    "                colorscale=\"Viridis\",\n",
    "                colorbar=dict(title=\"Attention Score\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add title and layout\n",
    "        fig.update_layout(\n",
    "            title=f\"{title} - Head {head + 1}\",\n",
    "            xaxis=dict(title=\"Tokens\", tickangle=45),\n",
    "            yaxis=dict(title=\"Tokens\"),\n",
    "        )\n",
    "\n",
    "        # Highlight attention label regions if provided\n",
    "        if attn_labels is not None:\n",
    "            for i in range(len(attn_labels)):\n",
    "                for j in range(len(attn_labels)):\n",
    "                    if attn_labels[i, j] == 1:\n",
    "                        fig.add_shape(\n",
    "                            type=\"rect\",\n",
    "                            x0=j - 0.5,\n",
    "                            x1=j + 0.5,\n",
    "                            y0=i - 0.5,\n",
    "                            y1=i + 0.5,\n",
    "                            line=dict(color=\"red\", width=2),\n",
    "                        )\n",
    "\n",
    "        # Save the plot if a save path is provided\n",
    "        if save_path:\n",
    "            fig.write_image(f\"{save_path}_head_{head + 1}.png\")\n",
    "\n",
    "        # Show the plot\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mask = None\n",
    "\n",
    "def compute_scores(logits, target):\n",
    "    ignore_index = -100  # Optional: index to ignore in loss/accuracy computation\n",
    "    # Compute loss\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "    logits.view(-1, logits.size(-1)),  # Flatten logits to (batch_size * seq_len, vocab_size)\n",
    "    target.view(-1),               # Flatten target_ids to (batch_size * seq_len)\n",
    "    ignore_index=ignore_index          # Ignore padding tokens if applicable\n",
    ")\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = torch.argmax(logits, dim=-1)  # Get predicted token indices\n",
    "    correct = (predictions == target) & (target != ignore_index)  # Ignore padding tokens\n",
    "    accuracy = correct.sum().item() / (target != ignore_index).sum().item()\n",
    "\n",
    "    print(f\"Loss: {loss.item()}, Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    errors = (predictions != target) & (target != ignore_index)  # Identify error positions\n",
    "    errors_np = errors.cpu().numpy()  # Convert to NumPy for visualization\n",
    "    #print(\"Errors:\", errors_np)\n",
    "\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "def randomly_expand_mask(mask, num_additional_trues):\n",
    "    \"\"\"\n",
    "    Randomly add more `True` values to a boolean mask in the lower triangle.\n",
    "\n",
    "    Args:\n",
    "        mask (torch.Tensor): A boolean tensor of shape (seq_len, seq_len) with `True` values.\n",
    "        num_additional_trues (int): Number of additional `True` values to add.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The expanded mask with more `True` values.\n",
    "    \"\"\"\n",
    "    seq_len = mask.shape[0]\n",
    "\n",
    "    # Ensure the mask is in the lower triangle\n",
    "    causal_mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool, device=mask.device))\n",
    "\n",
    "    # Find positions in the lower triangle that are currently `False`\n",
    "    available_positions = (~mask) & causal_mask\n",
    "    available_indices = torch.nonzero(available_positions, as_tuple=False)  # Shape: (num_available, 2)\n",
    "\n",
    "    # Randomly select a subset of these positions\n",
    "    if available_indices.size(0) > 0:\n",
    "        num_to_add = min(num_additional_trues, available_indices.size(0))\n",
    "        selected_indices = available_indices[torch.randperm(available_indices.size(0))[:num_to_add]]\n",
    "\n",
    "        # Set the selected positions to `True`\n",
    "        mask[selected_indices[:, 0], selected_indices[:, 1]] = True\n",
    "\n",
    "    return mask\n",
    "\n",
    "def fix_attn_pattern_hook(\n",
    "    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint,\n",
    "    attn_labels_1_idx,\n",
    "    heads_to_reinforce,\n",
    "    attn_labels,\n",
    "    keep_last_n_tokens,\n",
    "    threshold=3e-3,\n",
    "    expand=10\n",
    ") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
    "    global mask\n",
    "    batch, heads, seq_len, _ = value.shape\n",
    "\n",
    "    # Create a mask initialized to False\n",
    "    mask = torch.zeros((seq_len, seq_len), dtype=torch.bool, device=value.device)\n",
    "\n",
    "    # Set diagonal positions to True\n",
    "    mask |= torch.eye(seq_len, dtype=torch.bool, device=value.device)\n",
    "\n",
    "    # Set the first token (column 0) to True\n",
    "    mask[:, 0] = True\n",
    "\n",
    "    # Set positions in attn_labels_1_idx to True\n",
    "    for x, y in attn_labels_1_idx:\n",
    "        mask[x, y] = True\n",
    "\n",
    "    # Expand the mask to match the shape of attention_scores\n",
    "    mask = mask.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, seq_len)\n",
    "    mask = mask.expand(batch, heads, seq_len, seq_len)  # Shape: (batch, heads, seq_len, seq_len)\n",
    "    print(mask)\n",
    "    mask = randomly_expand_mask(mask, expand)\n",
    "    print(mask)\n",
    "    # Set positions not in the mask to -inf\n",
    "    value = value.masked_fill(~mask, float('-inf'))\n",
    "\n",
    "    # Compute the softmax probabilities after adjustment\n",
    "    softmax_scores_after = torch.softmax(value, dim=-1)\n",
    "    \"\"\"\n",
    "    # Verify that the probabilities of sink, diagonal, and reference tokens remain unchanged\n",
    "    for b in range(batch):\n",
    "        for h in range(heads):\n",
    "            for i in range(seq_len):\n",
    "                # Sink token (first column)\n",
    "                assert torch.isclose(\n",
    "                    softmax_scores_before[b, h, i, 0], softmax_scores_after[b, h, i, 0], atol=5e-1\n",
    "                ), f\"Sink token probability mismatch at batch {b}, head {h}, row {i}. Before: {softmax_scores_before[b, h, i, 0]} After: {softmax_scores_after[b, h, i, 0]}\"\n",
    "\n",
    "                # Diagonal token\n",
    "                assert torch.isclose(\n",
    "                    softmax_scores_before[b, h, i, i], softmax_scores_after[b, h, i, i], atol=5e-1\n",
    "                ), f\"Diagonal token probability mismatch at batch {b}, head {h}, row {i}. Before: {softmax_scores_before[b, h, i, 0]} After: {softmax_scores_after[b, h, i, 0]}\"\n",
    "\n",
    "                # Reference tokens\n",
    "                for x, y in attn_labels_1_idx:\n",
    "                    if x == i:  # Same row\n",
    "                        assert torch.isclose(\n",
    "                            softmax_scores_before[b, h, x, y], softmax_scores_after[b, h, x, y], atol=5e-1\n",
    "                        ), f\"Reference token probability mismatch at batch {b}, head {h}, row {x}, col {y}. Before: {softmax_scores_before[b, h, i, 0]} After: {softmax_scores_after[b, h, i, 0]}\"\n",
    "\n",
    "    \"\"\"\n",
    "    return value\n",
    "\n",
    "def vis_attn_pattern_hook(\n",
    "    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint,\n",
    "    attn_labels_1_idx,\n",
    "    heads_to_reinforce,\n",
    "    attn_labels,\n",
    "    keep_last_n_tokens,\n",
    "    threshold=3e-3\n",
    ") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
    "    batch, heads, seq_len, _ = value.shape \n",
    "    layer = int(hook.name.split(\".\")[1])\n",
    "    if layer not in heads_to_reinforce.keys():\n",
    "        return\n",
    "    \n",
    "    for h in range(heads):\n",
    "        if h not in heads_to_reinforce[layer]:\n",
    "            continue\n",
    "        acc = 0\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            for j in range(seq_len):\n",
    "                if (i, j) in attn_labels_1_idx and value[0, h, i, j] >= threshold:\n",
    "                    value[0, h, i, j] += 1\n",
    "                    #acc += value[0, h, i, j]\n",
    "                    pass\n",
    "        #best_heads[(layer, h)] = best_heads.get((layer, h), 0) + acc\n",
    "        #visualize_attention_heads(value, None, title=f\"Example Attention Heatmap Layer {layer}\", heads=[h], length=keep_last_n_tokens, attn_labels=attn_labels)\n",
    "\n",
    "    return value\n",
    "    pass\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "is_attn_scores = lambda name: name.endswith(\"hook_attn_scores\")\n",
    "is_attn_pattern = lambda name: name.endswith(\"hook_pattern\")\n",
    "ablated = []\n",
    "for idx in range(0, 30):\n",
    "    \n",
    "    indices =  np.argwhere(attn_labels[idx].cpu().numpy() == 1)\n",
    "    attn_labels_1_idx = set(map(tuple, indices))\n",
    "    \n",
    "\n",
    "    model.to(device)\n",
    "    input_ids[idx] = input_ids[idx].to(device)\n",
    "    target_ids[idx] = target_ids[idx].to(device)\n",
    "    attn_labels[idx] = attn_labels[idx].to(device)\n",
    "                \n",
    "\n",
    "    heads_to_reinforce_x=[\n",
    "        (7, [5]),\n",
    "        (8, [20, 31]),\n",
    "        (9, [8, 10]),\n",
    "        (10, [23, 20, 21, 1, 22]),\n",
    "        (11, [14, 20, 4]),\n",
    "        (12, [13, 2, 29]),\n",
    "        (13, [20, 21, 22, 2, 23, 31, 1, 11]),\n",
    "        (14, [22, 11, 20, 15, 21, 14, 31, 3, 28, 29]),\n",
    "        (15, [31, 7, 22, 30, 20, 29]),\n",
    "    ]\n",
    "    threshold = 0\n",
    "    cols_to_keep = ~np.all(attn_labels[idx].cpu().numpy() == -100, axis=0)\n",
    "    keep_last_n_tokens = 400\n",
    "    heads_x = dict(heads_to_reinforce_x)\n",
    "    logits = model.run_with_hooks(input_ids[idx], return_type=\"logits\",\n",
    "                                    fwd_hooks=[\n",
    "                                        (is_attn_scores, partial(fix_attn_pattern_hook, attn_labels_1_idx=attn_labels_1_idx, heads_to_reinforce=heads_x, attn_labels=attn_labels[idx], keep_last_n_tokens = keep_last_n_tokens, expand=5)),\n",
    "                                        (is_attn_pattern, partial(vis_attn_pattern_hook, attn_labels_1_idx=attn_labels_1_idx, heads_to_reinforce=heads_x, attn_labels=attn_labels[idx], keep_last_n_tokens = keep_last_n_tokens))\n",
    "                                    \n",
    "                                    ])\n",
    "    loss, acc = compute_scores(logits, target_ids[idx])\n",
    "    ablated.append((heads_x, loss, acc))\n",
    "    del logits\n",
    "    torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = 0\n",
    "for item in ablated:\n",
    "    sums += item[2]\n",
    "print(sums/len(ablated))\n",
    "print(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in sorted(best_heads.items(), key=lambda item: item[1], reverse=True):\n",
    "    print(item[0][0], item[0][1], item[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
