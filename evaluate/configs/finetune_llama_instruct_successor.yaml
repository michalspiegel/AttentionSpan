# configs/transfomers_config.yaml
defaults:
  - _self_

wandb:
  project: algorithmic_reasoning
  name: llama_instruct_1B_successor_1_30_50_90_finetuned

model:
  name: meta-llama/Llama-3.2-1B-Instruct
  load_from_checkpoint: False
  checkpoint_filepath: checkpoints/llama_instruct_successor_finetuned.weights
  is_pretrained: True
  model_configuration:
    attention_probs_dropout_prob: 0.0
    hidden_dropout_prob: 0.0
    
training:
  epochs: 1
  batch_size: 4
  learning_rate: 5e-6
  device: cuda
  ignore_index: -100
  save_checkpoint: True
  checkpoint_filepath: checkpoints/llama_instruct_successor_finetuned.weights
  optimizer_parameters:
    beta1: 0.95
    beta2: 0.999
    weight_decay: 0.2
  train_dataset:
    generator: SuccessorGenerator
    generator_parameters:
      seed: 0
      use_instruction: True
      use_few_shot: True
      start_number_range: ${as_tuple:1,90}
      length: ${as_tuple:2,4}
      generate_attn_labels: False
      apply_chat_template: True
    num_samples: 6000
  val_dataset:
    generator: SuccessorGenerator
    generator_parameters:
      seed: 0
      use_instruction: True
      use_few_shot: True
      start_number_range: ${as_tuple:1,200}
      length: ${as_tuple:2,6}
      generate_attn_labels: False
      apply_chat_template: True
    num_samples: 100


evaluation:
  device: cuda
  batch_size: 16
  visualize: False
  name: llama_instruct_1B_successor_OOD_5_6_50_90
  dataset1:
    num_samples: 100
    generator: SuccessorGenerator
    generator_parameters:
      seed: 42
      use_instruction: True
      use_few_shot: True
      start_number_range: ${as_tuple:1,90}
      length: ${as_tuple:2,4}
      apply_chat_template: True
  dataset2:
    num_samples: 500
    generator: SuccessorGenerator
    generator_parameters:
      seed: 42
      use_instruction: True
      use_few_shot: True
      start_number_range: ${as_tuple:100, 900}
      length: ${as_tuple:5,6}
      apply_chat_template: True
  