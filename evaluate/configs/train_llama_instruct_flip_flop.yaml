# configs/transfomers_config.yaml
defaults:
  - _self_

wandb:
  project: algorithmic_reasoning
  name: llama_1B_instruct_flip_flop_10_reg_10_50_len_flip_trained

model:
  name: meta-llama/Llama-3.2-1B-Instruct
  load_from_checkpoint: False
  checkpoint_filepath: checkpoints/llama_instruct_flip_flop_trained.weights
  is_pretrained: False
  model_configuration:
    attention_probs_dropout_prob: 0.0
    hidden_dropout_prob: 0.0

training:
  epochs: 1
  batch_size: 4
  learning_rate: 5e-6
  device: cuda
  ignore_index: -100
  save_checkpoint: True
  checkpoint_filepath: checkpoints/llama_instruct_flip_flop_trained.weights
  optimizer_parameters:
    beta1: 0.95
    beta2: 0.999
    weight_decay: 0.2
  train_dataset:
    generator: FlipFlopGenerator
    generator_parameters:
      seed: 0
      length: ${as_tuple:1,10}
      use_flip: True
      num_registers: ${as_tuple:2,2}
      generate_attn_labels: False
    num_samples: 6000
  val_dataset:
    generator: FlipFlopGenerator
    generator_parameters:
      seed: 0
      length: ${as_tuple:1,20}
      use_flip: True
      num_registers: ${as_tuple:2,2}
      generate_attn_labels: False
    num_samples: 100


evaluation:
  device: cuda
  batch_size: 16
  visualize: False
  name: llama_1B_instruct_flip_flop_trained
  dataset1:
    num_samples: 100
    generator: FlipFlopGenerator
    generator_parameters:
      seed: 42
      length: ${as_tuple:1,10}
      use_flip: True
      num_registers: ${as_tuple:2,2}
  dataset2:
    num_samples: 500
    generator: FlipFlopGenerator
    generator_parameters:
      seed: 42
      length: ${as_tuple:11,20}
      use_flip: True
      num_registers: ${as_tuple:2,2}

  