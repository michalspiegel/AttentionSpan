# configs/transfomers_config.yaml
defaults:
  - _self_

wandb:
  project: algorithmic_reasoning
  name: llama_1B_instruct_multiplication_1-5_6-10_digits_trained

model:
  name: meta-llama/Llama-3.2-1B-Instruct
  load_from_checkpoint: False
  checkpoint_filepath: checkpoints/llama_instruct_multiplication_trained.weights
  is_pretrained: False
  model_configuration:
    attention_probs_dropout_prob: 0.0
    hidden_dropout_prob: 0.0
    
training:
  epochs: 1
  batch_size: 4
  learning_rate: 5e-6
  device: cuda
  ignore_index: -100
  save_checkpoint: True
  checkpoint_filepath: checkpoints/llama_instruct_multiplication_finetuned.weights
  optimizer_parameters:
    beta1: 0.95
    beta2: 0.999
    weight_decay: 0.2
  train_dataset:
    generator: LongMultiplicationGenerator
    generator_parameters:
      seed: 0
      length: ${as_tuple:1,3}
      generate_attn_labels: False
    num_samples: 6000
  val_dataset:
    generator: LongMultiplicationGenerator
    generator_parameters:
      seed: 0
      length: ${as_tuple:1,5}
      generate_attn_labels: False
    num_samples: 100


evaluation:
  device: cuda
  batch_size: 1
  visualize: False
  name: llama_1B_instruct_multiplication_trained
  dataset1:
    num_samples: 100
    generator: LongMultiplicationGenerator
    generator_parameters:
      seed: 42
      length: ${as_tuple:1,3}
  dataset2:
    num_samples: 500
    generator: LongMultiplicationGenerator
    generator_parameters:
      seed: 42
      length: ${as_tuple:4,5}


  